[ { "title": "Managing K8S secrets with ArgoCD and SOPS", "url": "/posts/secrets-sops-argo/", "categories": "cloud, k8s", "tags": "cloud, k8s, argocd, sops", "date": "2023-05-15 16:00:00 +0200", "snippet": "ArgoCD is a popular tool for managing Kubernetes resources, providing a declarative way to define and maintain the desired state of your applications running in a Kubernetes cluster. One essential ...", "content": "ArgoCD is a popular tool for managing Kubernetes resources, providing a declarative way to define and maintain the desired state of your applications running in a Kubernetes cluster. One essential aspect of deploying applications securely is managing secrets. Kubernetes provides the concept of secrets, but handling them effectively within ArgoCD can be a challenge. This is where Ksops comes to the rescue. In this blog post, we will explore how to use Ksops to manage secrets in ArgoCD, ensuring a more secure and efficient deployment process.Understanding ArgoCD SecretsBefore diving into the details of Ksops, let’s briefly understand how ArgoCD manages secrets. ArgoCD uses a GitOps approach, storing application configurations in a Git repository. While application manifests can be version-controlled and audited, secrets require extra care due to their sensitive nature.ArgoCD offers several options for managing secrets: Git-Secrets: Secrets can be stored in a Git repository, encrypted using sops or a similar tool. However, this may not be the most secure option, as it still exposes secrets to anyone with access to the repository. External Secret Management: You can use an external secret management tool such as HashiCorp Vault, Kubernetes secrets, or a cloud provider’s secret manager. While this approach provides better security, it can be more complex to set up and manage. Enter KsopsKsops (Kustomize-SOPS) is a powerful tool designed to simplify and secure the management of secrets in Kubernetes, especially in GitOps workflows like ArgoCD. Ksops takes a declarative approach to secret management, allowing you to store encrypted secrets alongside your application manifests in the same Git repository. It offers the following benefits: Zero Trust Secrets: With Ksops, secrets are stored in the same Git repository as your application manifests, but they are securely encrypted. This means you don’t have to rely on the secrecy of your Git repository, following the “Zero Trust” principle. Ease of Use: Ksops integrates seamlessly with ArgoCD and other GitOps tools, making it easy to incorporate into your existing workflow. Strong Encryption: Ksops uses strong encryption algorithms to protect your secrets, ensuring they remain confidential. Audit Trail: Since secrets are version-controlled in the Git repository, you can maintain a clear audit trail of who made changes to the secrets. Installing SOPSIn order to install SOPS, make sure you have pip installed on your environment, and then download the SOPS package with:$ pip install sopsConfiguring SOPS (Secrets Manager)Now that we have SOPS installed on our machine, let’s look on how to configure it to use our personal GPG keysGenerate GPG Keys$ export GPG_NAME=\"k3s.argotest.cluster\"$ export GPG_COMMENT=\"argocd secrets\"$ gpg --batch --full-generate-key &lt;&lt;EOF%no-protectionKey-Type: 1Key-Length: 4096Subkey-Type: 1Subkey-Length: 4096Expire-Date: 0Name-Comment: ${GPG_COMMENT}Name-Real: ${GPG_NAME}EOFWith the following command, we create a new GPG key of 4096 bytes, and add a comment and name to it (in order to be able to distinguish it easily from others)Retrieve key name$ gpg --list-secret-keys \"${GPG_NAME}\"This command will output you the ID of your GPG key, which will be used from now on to identiy your GPG key.Store the GPG key fingerprint as an environment variable$ export GPG_ID=&lt;OUTPUT_FROM_PREVIOUS_COMMAND&gt;Export the public and private key pair from the GPG key and create a kubernetes secret for ArgoCD to read them$ gpg --export-secret-keys --armor \"${GPG_ID}\" |kubectl create secret generic sops-gpg \\--namespace=argocd \\--from-file=sops.asc=/dev/stdinMake sure to replace argocd for the namespace in which argo is being deployedStore the public key (so other people can encrypt secrets) gpg --export --armor \"${GPG_ID}\" &gt; .sops.pub.ascAnd the key can be imported by other people with:&gt; gpg --import .sops.pub.asc`Configure ArgoCD with KSOPSOn ArgoCD configmap (argocd-cm), add the following key and value: kustomize.buildOptions: –enable-alpha-pluginsPatching ArgoCD Repo serverPatch the Argo CD repo server deployment with the following contents:apiVersion: apps/v1kind: Deploymentmetadata: name: argocd-repo-server namespace: argocdspec: template: spec: initContainers: - name: install-ksops image: viaductoss/ksops:v4.2.5 command: - /bin/sh - '-c' args: - &gt;- echo \"Installing KSOPS...\"; mv ksops /custom-tools/; mv kustomize/custom-tools/; echo \"Done.\"; volumeMounts: - mountPath: /custom-tools name: custom-tools - name: import-gpg-key image: argoproj/argocd:v2.1.7 command: [\"gpg\", \"--import\",\"/sops-gpg/sops.asc\"] env: - name: GNUPGHOME value: /gnupg-home/.gnupg volumeMounts: - mountPath: /sops-gpg name: sops-gpg - mountPath: /gnupg-home name: gnupg-home containers: - name: argocd-repo-server env: - name: XDG_CONFIG_HOME value: /.config - name: GNUPGHOME value: /home/argocd/.gnupg volumeMounts: - mountPath: /home/argocd/.gnupg name: gnupg-home subPath: .gnupg - mountPath: /usr/local/bin/kustomize name: custom-tools subPath: kustomize - mountPath: /.config/kustomize/plugin/viaduct.ai/v1/ksops/ksops name: custom-tools subPath: ksops volumes: - name: custom-tools emptyDir: {} - name: gnupg-home emptyDir: {} - name: sops-gpg secret: secretName: sops-gpgWhat are we doing here? We are settign up an initContainer for our ArgoCD deployment, which contains: The KSOPS installer The GPG Key importer Patch it with:$ kubectl patch deployment -n argocd argocd-repo-server --patch \"$(cat repo-deploy-patch.yaml)Creating a KSOPS secrets generatorInside the Kustmize overlay (info here), create a secret generator object:apiVersion: viaduct.ai/v1kind: ksopsmetadata: # Specify a name name: example-secret-generatorfiles: - ./argo-secret.yamlWhere argo-secret refers to the secret to be decrypted by ksops.With this done, reference the generator inside the kustomization.yaml file:generators: - ./secrets-generator.yamlConclusionAnd voilà! With this setup, everytime you need to encrypt a secret, you just need to: Encrypt the file with sops --encrypt &lt;filename&gt; (for example, a secrets.yaml file) Add the file reference to the secrets-generator.yaml file Commit the encrypted file to your Git Repo ArgoCD will take care of leveraging KSOPS for decrypting the secrets content" }, { "title": "HIPAA on AWS: S3 buckets", "url": "/posts/hipaa-s3/", "categories": "cloud, regulatory", "tags": "aws, s3, hipaa", "date": "2023-04-15 16:00:00 +0200", "snippet": "Storing healthcare data safely is a big and relevant topic, and HIPAA (the Health Insurance Portability and Accountability Act) has some strict rules about it. Nowadays, many healthcare organizatio...", "content": "Storing healthcare data safely is a big and relevant topic, and HIPAA (the Health Insurance Portability and Accountability Act) has some strict rules about it. Nowadays, many healthcare organizations are turning to Amazon S3 buckets in the cloud to keep patient records and other health-related info secure. Let’s break down how you can use Amazon S3 buckets to follow HIPAA standards and keep your healthcare data safe.Disable Acess Control Lists (ACLs)When ACLs are disabled access control for your data is based on policies, such as the following: AWS Identity and Access Management (IAM) user policies S3 bucket policies Virtual private cloud (VPC) endpoint policies AWS Organizations service control policiesresource \"aws_s3_bucket\" \"example_bucket\" { bucket = \"your-bucket-name\" acl = \"private\"}Use IAM roles for accessing S3 bucketsAvoid directly using AWS credentias (access key + token), and instead, use IAM roles for accessing S3 buckets Fine-graind access control: IAM buckets allows us to precisely define which roles can access which specific resources Least privile principle: By default IAM roles will follow leas privilege principle, making sure we are not providing more permissions that what is needed Temporary credentials: by using a role instead of a pair of long-living keys, we minimize the risk of long-term credential exposureExample using an EC2 instance that accesses a S3 bucket through an IAM role:# Create an S3 bucketresource \"aws_s3_bucket\" \"example_bucket\" { bucket = \"your-s3-bucket-name\" acl = \"private\"}# Create an IAM role for EC2 instancesresource \"aws_iam_role\" \"s3_access_role\" { name = \"EC2S3AccessRole\" assume_role_policy = jsonencode({ Version = \"2012-10-17\", Statement = [ { Action = \"sts:AssumeRole\", Effect = \"Allow\", Principal = { Service = \"ec2.amazonaws.com\" } } ] })}# Attach an S3 access policy to the IAM roledata \"aws_iam_policy_document\" \"s3_access_policy\" { statement { actions = [\"s3:GetObject\", \"s3:PutObject\", \"s3:ListBucket\"] resources = [\"arn:aws:s3:::your-s3-bucket-name/*\"] }}resource \"aws_iam_policy\" \"s3_access_policy\" { name = \"S3AccessPolicy\" description = \"Policy for S3 access\" policy = data.aws_iam_policy_document.s3_access_policy.json}resource \"aws_iam_policy_attachment\" \"s3_access_policy_attachment\" { name = \"s3_access_policy_attachment\" policy_arn = aws_iam_policy.s3_access_policy.arn roles = [aws_iam_role.s3_access_role.name]}# Launch an EC2 instance with the IAM roleresource \"aws_instance\" \"example_instance\" { ami = \"ami-0c55b159cbfafe1f0\" # Specify a valid AMI ID instance_type = \"t2.micro\" subnet_id = \"subnet-0123456789abcdef0\" # Specify a valid subnet ID key_name = \"your-key-name\" # Specify your key pair iam_instance_profile = aws_iam_role.s3_access_role.name tags = { Name = \"ExampleInstance\" }}Using Object Lock Data retention and preservation: ensures that cannot be deleted or modified until a specific reteiton period is set Data immutability: prevents unauthorized changes to data, enhancing data integrity and compliance Accidental deletion prevention: makes sure that not even administrators can accidentaly remove sensitive data, reducing data loss riskresource \"aws_s3_bucket\" \"hipaa_bucket\" { bucket = \"your-hipaa-compliant-bucket\" acl = \"private\" versioning { enabled = true } object_lock_configuration { object_lock_enabled = \"Enabled\" }}Data Encryption In-transit: as it travels to and from AWS S3 SSL/TLS or client-side encryption At rest: while stored on S3 data centers Server-side encryption: S3 encrypts your objects before saving on disk, and then decrypts when you need to download them back All S3 buckets have encryption configured by default (using SSE-S3) Customizing server-side encryption: Server-side encryption with AWS S3 managed keys (SSE-S3) Open S3 bucket on console Go to properties Scroll down to server-side encryption settings Pick “override bucket default encryption settings On “encryption type”, chose AWS S3 managed keys (SSE-S3) Server-side encryption with AWS KMS (SSE-KMS) Same as the first (SSE-S3), but on encryption type, choose AWS Key Management Service Keys (SSE-KMS) Dual-layer server-side encryption with AWS KMS keys (DSSE-KMS): Same as the first one, but on encryption type, choose Dual-Layer server-side encryption with AWS KMS (DSSE-KMS) Under AWS KMS key, select an existing KMS key from the available keys (or create one if not existent) Server-side encryption with customer-provided keys At the time of the object creation (e.g. REST API), specify the encryption key you want to use with the following HTTP headers: x-amz-server-side​-encryption​-customer-algorithm: specify the encryption algorithm. Must be AES256 x-amz-server-side​-encryption​-customer-key: specify the 256-bit, base64 encoded encryption key for S3 to encrypt/decrypt the data x-amz-server-side​-encryption​-customer-key-MD5: specify the base64-encoded 128-bit MD5 digest of the encryption key. This header is used to make sure the integrity key was transmitted without error Client-side encryption: You take care of encrypting your data, sending to S3, and then decrypt when downloading it back You can use the AWS S3 Encryption Client, which offers the possibility to perform client-side encryption before sending data and retrieving data to/from S3. More info here: https://docs.aws.amazon.com/amazon-s3-encryption-client/latest/developerguide/what-is-s3-encryption-client.htmlExample of server-side encryption using SSE-S3:resource \"aws_s3_bucket\" \"hipaa_bucket\" { bucket = \"your-hipaa-bucket-name\" # Enable server-side encryption with Amazon S3-managed keys (SSE-S3) server_side_encryption_configuration { rule { apply_server_side_encryption_by_default { sse_algorithm = \"AES256\" } } }AWS Service Endpoints Connections to S3 containing PHI (Patient Health Information) must use encrypted transport methods (HTTPS).Naming conventionsDo not store PHI in: Bucket names Object names MetadataWhy? Because this data is not encrypted using server-side encryption and usually not encrypted on client-side neither.VersioningWhy to enable versioning on S3 buckets? Data Integrity: protects from accidental or malicious data deletions or modifications Audit trail: provides an audit trail of changes made to objects on bucket Retention requirements: HIPAA, as other regulatory entities, mandates specific data retentions. Versioning allows you to maintain all object versions Recovery and rollback: Allows you to recover data to previous versions in case of corruption or unintended changesDoing it with Terraform:// inside your TF bucket definitionversioning { enabled = true}Access Logging Audit trails: who accessed what when, aiding in auditing and compliance Security monitoring: helps on identifying suspicious or unauthorized access attempts Compliance requirements: needed for regulatory compliance standards (HIPAA, GDPR) Incident response: in case of data breach, access logs can be essential to understand the scope of the breach Policy enforcement: making sure the access policies are properly configured and enforcedTerraform code:# Create the source S3 bucketresource \"aws_s3_bucket\" \"source_bucket\" { bucket = \"your-source-bucket-name\"}# Create the target S3 bucket for access logsresource \"aws_s3_bucket\" \"access_logging_bucket\" { bucket = \"your-access-logs-bucket-name\"}# Enable access logging for the source bucketresource \"aws_s3_bucket_logging\" \"access_logging\" { bucket = aws_s3_bucket.source_bucket.id target_bucket = aws_s3_bucket.access_logging_bucket.id target_prefix = \"access-logs/\"}Cross-Region Replication Data redundancy: data stored on multiple geographic regions reduces the risk of data loss due to disasters or hardware failures Data availability: downtime is reduced when one geographic region fails, ensuring customers can access patient data when needed Disaster recovery: in case of accidental or malicious agents, data can still be recovered Integrity: allows to check the integrity/consistency of the dataprovider \"aws\" { region = \"us-east-1\" # The source region}# Create the source S3 bucketresource \"aws_s3_bucket\" \"source_bucket\" { bucket = \"your-source-bucket-name\" acl = \"private\"}provider \"aws\" { alias = \"us-west-2\" # The target region region = \"us-west-2\"}# Create the target S3 bucket in a different regionresource \"aws_s3_bucket\" \"target_bucket\" { provider = aws.us-west-2 bucket = \"your-target-bucket-name\" acl = \"private\"}# Enable versioning in both source and target bucketsresource \"aws_s3_bucket_versioning\" \"source_versioning\" { bucket = aws_s3_bucket.source_bucket.id}resource \"aws_s3_bucket_versioning\" \"target_versioning\" { provider = aws.us-west-2 bucket = aws_s3_bucket.target_bucket.id}# Configure cross-region replication for the source bucketresource \"aws_s3_bucket_replication_configuration\" \"replication_config\" { bucket = aws_s3_bucket.source_bucket.id role = \"arn:aws:iam::YOUR_ACCOUNT_ID:role/your-replication-role\" destination { bucket = aws_s3_bucket.target_bucket.id } rules { id = \"rule-1\" status = \"Enabled\" prefix = \"\" }}Using VPC access for S3 access Network isolation: by using a VPC, you ensure that your data does not get exposed to external threats and it is available only inside the AWS network Data control: allows organizations to maintain full control over data accessLeverage Amazon Macie to discover sensitive data Amazon Macie is a security service that discovers sensitive data by using machine learning and pattern matchingMore info here: https://docs.aws.amazon.com/macie/latest/user/what-is-macie.htmlReferences Architecting for HIPAA Security and Compliance on Amazon Web Services Whitepaper AWS S3 Documentation - Protecting data with encryption Logging and monitoring in Amazon S3 Resilience in Amazon S3 Security best practices for Amazon S3" }, { "title": "NLP with Python: A Beginner's Guide", "url": "/posts/nlp-basics/", "categories": "data science, nlp", "tags": "data science, nlp, python", "date": "2023-04-15 16:00:00 +0200", "snippet": "Natural Language Processing (NLP) is a fascinating field that empowers computers to understand, interpret, and generate human language. Python, with its rich ecosystem of libraries and tools, is a ...", "content": "Natural Language Processing (NLP) is a fascinating field that empowers computers to understand, interpret, and generate human language. Python, with its rich ecosystem of libraries and tools, is a fantastic choice for NLP tasks. In this blog post, we’ll explore essential techniques for NLP in Python, including data cleaning, stemming, preprocessing, and extracting meaning from text data. These techniques will help you unlock the potential of NLP for various applications, from sentiment analysis to chatbots and language translation.1. Data CleaningBefore you can derive meaningful insights from text data, you need to ensure your data is clean and free from noise. Common data cleaning techniques include: Removing HTML tags: If you’re dealing with web data, use libraries like BeautifulSoup to strip out HTML tags Handling special characters: Remove or replace special characters, such as punctuation marks Lowercasing: Convert all text to lowercase to ensure consistency Dealing with numbers: Decide whether to keep or remove numbers (depends on use-case)import redef clean_text(text): # Remove HTML tags text = re.sub(r'&lt;.*?&gt;', '', text) # Remove special characters and digits text = re.sub(r'[^a-zA-Z\\s]', '', text) # Convert to lowercase text = text.lower() return text# Example usagedirty_text = \"&lt;p&gt;This is an example text with 123 special characters!&lt;/p&gt;\"cleaned_text = clean_text(dirty_text)print(\"Input Text: \", dirty_text)print(\"Cleaned Text: \", cleaned_text)Output:Input Text: &lt;p&gt;This is an example text with 123 special characters!&lt;/p&gt;Cleaned Text: this is an example text with special characters2. TokenizationTokenization is the process of breaking down text into individual words or tokens. In Python, libraries like NLTK and spaCy are popular choices for tokenization. Tokenization allows you to perform various operations at the token level.import nltkfrom nltk.tokenize import word_tokenizenltk.download('punkt')text = \"Tokenization is a fundamental NLP technique.\"tokens = word_tokenize(text)# Example usageprint(\"Input Text: \", text)print(\"Tokens: \", tokens)Output:Input Text: Tokenization is a fundamental NLP technique.Tokens: ['Tokenization', 'is', 'a', 'fundamental', 'NLP', 'technique', '.']3. Stopword RemovalStopwords are common words (e.g., “and,” “the,” “in”) that don’t carry significant meaning in text analysis. Removing stopwords can reduce the dimensionality of your data and improve the performance of NLP models.from nltk.corpus import stopwordsnltk.download('stopwords')text = \"This is an example sentence with some stopwords.\"stop_words = set(stopwords.words('english'))filtered_words = [word for word in word_tokenize(text) if word.lower() not in stop_words]# Example usageprint(\"Input Text: \", text)print(\"Filtered Words: \", filtered_words)Output:Input Text: This is an example sentence with some stopwords.Filtered Words: ['example', 'sentence', 'stopwords', '.']4. Stemming and LemmatizationStemming and lemmatization are techniques to reduce words to their base or root form. Stemming is a more aggressive approach, while lemmatization considers the context of words for transformation.from nltk.stem import PorterStemmer, WordNetLemmatizerstemmer = PorterStemmer()lemmatizer = WordNetLemmatizer()word = \"running\"stemmed_word = stemmer.stem(word)lemmatized_word = lemmatizer.lemmatize(word)# Example usageprint(\"Input Word: \", word)print(\"Stemmed Word: \", stemmed_word)print(\"Lemmatized Word: \", lemmatized_word)Output:Input Word: runningStemmed Word: runLemmatized Word: running5. Part-of-Speech TaggingPart-of-speech (POS) tagging is the process of labeling words in a text as nouns, verbs, adjectives, and more. POS tagging can help in understanding the grammatical structure of sentences, which is useful in various NLP tasks.import nltkfrom nltk import pos_tagfrom nltk.tokenize import word_tokenizenltk.download('punkt')nltk.download('averaged_perceptron_tagger')text = \"NLP is fascinating.\"tokens = word_tokenize(text)pos_tags = pos_tag(tokens)# Example usageprint(\"Input Text: \", text)print(\"POS Tags: \", pos_tags)Output:Input Text: NLP is fascinating.POS Tags: [('NLP', 'NNP'), ('is', 'VBZ'), ('fascinating', 'VBG'), ('.', '.')]6. Named Entity Recognition (NER)NER is the process of identifying and classifying named entities, such as names of people, places, organizations, and more, within text data. Libraries like spaCy offer pre-trained models for NER.import spacynlp = spacy.load(\"en_core_web_sm\")text = \"Apple Inc. is a technology company based in Cupertino, California.\"doc = nlp(text)# Example usageprint(\"Input Text: \", text)for entity in doc.ents: print(\"Entity: \", entity.text, \"Label: \", entity.label_)Output:Input Text: Apple Inc. is a technology company based in Cupertino, California.Entity: Apple Inc. Label: ORGEntity: Cupertino Label: GPEEntity: California Label: GPE7. Feature EngineeringFeature engineering is a crucial step in NLP, involving the creation of numerical features from text data. Common techniques include: Bag of Words (BoW): Representing text as a vector of word frequencies. Term Frequency-Inverse Document Frequency (TF-IDF): Assigning scores to words based on their importance in a document relative to a corpus. Word Embeddings: Techniques like Word2Vec and GloVe generate dense vector representations of words, capturing semantic relationships. from sklearn.feature_extraction.text import TfidfVectorizercorpus = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\", \"Is this the first document?\"]vectorizer = TfidfVectorizer()X = vectorizer.fit_transform(corpus)# Example usageprint(\"Input Corpus: \", corpus)print(\"TF-IDF Matrix: \")print(X.toarray())Output:Input Corpus: ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']TF-IDF Matrix:[[0. 0. 0.68091856 0.51785612 0.51785612 0. ] [0. 0. 0. 0.68091856 0.51785612 0.51785612] [0.68091856 0. 0.51785612 0. 0.51785612 0. ] [0. 0.68091856 0.51785612 0.51785612 0. 0. ]]8. Sentiment AnalysisSentiment analysis is the process of determining the sentiment or emotion expressed in text, such as positive, negative, or neutral. Python libraries like TextBlob and VADER Sentiment Analysis provide pre-trained models for this purpose.from textblob import TextBlobtext = \"Python is a great language!\"analysis = TextBlob(text)# Example usageprint(\"Input Text: \", text)print(\"Sentiment: \", analysis.sentiment)Output:Input Text: Python is a great language!Sentiment: Sentiment(polarity=0.8, subjectivity=0.75)" }, { "title": "Essential kubectl Commands", "url": "/posts/k8s-commands/", "categories": "devops", "tags": "devops, cloud, sysadmin, k8s", "date": "2023-04-15 16:00:00 +0200", "snippet": "kubectl is the Swiss Army knife of Kubernetes, providing several commands to interact with your K8s cluster. In this blog post, we’ll explore a range of kubectl commands for various day-to-day oper...", "content": "kubectl is the Swiss Army knife of Kubernetes, providing several commands to interact with your K8s cluster. In this blog post, we’ll explore a range of kubectl commands for various day-to-day operations1. List Resources List all resources: kubectl get all List resources in a namespace: kubectl get all -n my-namespace List all resources in all namespaces: kubectl get all --all-namespaces2. Resource Inspection Inspect a resource: kubectl describe pod my-pod View resource config in YAML format: kubectl get pod my-pod -o yaml3. Troubleshooting View container logs: kubectl logs my-pod Start an interactive shell in a container: kubectl exec -it my-pod -- /bin/sh Check resource events: kubectl describe pod my-pod4. Scaling and Updates Scale replicas for a deployment: kubectl scale --replicas=5 deployment/my-deployment Perform a rolling update: kubectl set image deployment/my-deployment my-container=new-image:tag5. Contexts and Switching List available contexts: kubectl config get-contexts Switch to a context: kubectl config use-context my-context6. Resource Deletion Delete a resource: kubectl delete pod my-pod Delete resources in a namespace: kubectl delete all --all -n my-namespace Delete all resources in all namespaces (with caution): kubectl delete all --all --all-namespaces7. Port Forwarding Forward a local port to a service: kubectl port-forward service/my-service 8080:808. Accessing Logs Access logs for a previous container: kubectl logs -p my-pod9. Getting Resource Status Check resource status in real-time: kubectl get pods -n my-namespace -w10. Executing Commands in a Container Run a command in a container: kubectl exec my-pod -- ls /app11. Copying Files Copy files from a container: kubectl cp my-pod:/path/to/source /path/to/destination12. Resource Modification Edit a resource configuration in the default editor: kubectl edit pod my-pod Apply changes to a resource without needing to delete and recreate: kubectl apply -f my-updated-pod.yaml13. Rollout Status Check rollout status of a deployment: kubectl rollout status deployment/my-deployment14. Rollback Deployments Roll back a deployment to a previous revision: kubectl rollout undo deployment/my-deployment15. Secrets and ConfigMaps Create a Secret: kubectl create secret generic my-secret --from-literal=secret-key=secret-value Create a ConfigMap: kubectl create configmap my-configmap --from-literal=config-key=config-value16. Access Control Create a Role: kubectl create role my-role --verb=get,list,create --resource=pods,pods/log --namespace=my-namespace Create a RoleBinding: kubectl create rolebinding my-binding --role=my-role --user=my-user17. Resource Labeling Label a resource (e.g., a pod named my-pod): kubectl label pod my-pod my-label=my-value18. Node Troubleshooting Describe a node to inspect its details: kubectl describe node my-node19. Service Exposing Expose a deployment as a service: kubectl expose deployment my-deployment --port=80 --target-port=80 --type=LoadBalancer20. Custom Columns in Output Display custom columns in resource listings: kubectl get pods -o custom-columns=NAME:.metadata.name,IMAGE:.spec.containers[*].image" }, { "title": "Simplifying Kubernetes Operators with Kopf: A Quick Guide", "url": "/posts/kopf-k8s-operator/", "categories": "devops", "tags": "devops, cloud, sysadmin, k8s", "date": "2023-03-15 15:00:00 +0100", "snippet": "Kubernetes Operators are powerful tools for managing complex applications in Kubernetes, but they can be challenging to develop and maintain. Here you can find the white paper for the Operator patt...", "content": "Kubernetes Operators are powerful tools for managing complex applications in Kubernetes, but they can be challenging to develop and maintain. Here you can find the white paper for the Operator pattern, if you want to know more about it.In this blog post, we’ll explore how Kopf, a Python framework designed to make building and managing Kubernetes Operators easier, that simplifies the creation of a Kubernetes Operator, by creating a basic example that deploys a Node.js application with an Nginx Ingress Controller.What is Kopf?Kopf is an open-source Python framework for building Kubernetes Operators. It abstracts many of the complexities of operator development, allowing you to focus on your application’s specific logic and behaviors.Setting up Kopf: Getting StartedBefore diving into our example, let’s set up a Python environment and install Kopf:# Create a Python virtual environmentpython3 -m venv venv# Activate the virtual environmentsource venv/bin/activate# Install Kopfpip install kopfNow, we’re ready to start building our Kubernetes Operator.Creating a Simple Node.js Application OperatorLet’s create a Kubernetes Operator that deploys a Node.js application and configures an Nginx Ingress Controller. We’ll start with the core Python code for the operator:import kopfimport kubernetes.client as k8sfrom kubernetes.client import V1ObjectMeta, V1Deployment, V1Service, V1Ingress, V1ConfigMap@kopf.on.create('myapp.example.com', 'v1', 'nodejs')def create_fn(name, spec, **kwargs): # Define Kubernetes resources deployment = V1Deployment( metadata=V1ObjectMeta(name=name), spec=spec['deployment'] ) service = V1Service( metadata=V1ObjectMeta(name=name), spec=spec['service'] ) ingress = V1Ingress( metadata=V1ObjectMeta(name=name), spec=spec['ingress'] ) configmap = V1ConfigMap( metadata=V1ObjectMeta(name=name), data={'DB_HOST': spec['database']['host']} ) # Create resources api = k8s.ApiClient() k8s.AppsV1Api(api).create_namespaced_deployment('default', deployment) k8s.CoreV1Api(api).create_namespaced_service('default', service) k8s.ExtensionsV1beta1Api(api).create_namespaced_ingress('default', ingress) k8s.CoreV1Api(api).create_namespaced_config_map('default', configmap)This Python code defines a function that handles the creation of our Node.js application along with the Nginx Ingress Controller.Defining the Custom Resource (CRD)To use our operator, we need to define a Custom Resource Definition (CRD). Create a YAML file (e.g., nodejs_app_crd.yaml) with the following content:apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata: name: nodejsapps.myapp.example.comspec: scope: Namespaced group: myapp.example.com names: plural: nodejsapps singular: nodejsapp kind: NodeJSApp versions: - name: v1 served: true storage: true subresources: status: {}Apply the CRD:kubectl apply -f nodejs_app_crd.yamlDeploying the OperatorNow, it’s time to deploy our operator. Create a Python file (e.g., operator.py) with the following content:import kopf@kopf.daemon('myapp.example.com', 'v1', annotations={'config.k8s.io/function': 'kopf'})def main(): pass # This function does nothing, it serves as the entry point for the operatorRun your operator:kopf run operator.pyYour Kopf operator is now up and running, waiting for custom resources to be created.Deploying a Node.js ApplicationCreate a custom resource for your Node.js application. Create a YAML file (e.g., nodejs_app_cr.yaml) with the following content:apiVersion: myapp.example.com/v1kind: NodeJSAppmetadata: name: my-nodejs-appspec: deployment: replicas: 2 template: spec: containers: - name: nodejs-app image: your-nodejs-image:latest service: selector: app: my-nodejs-app ports: - name: http port: 80 ingress: rules: - host: my-nodejs-app.example.com http: paths: - backend: serviceName: my-nodejs-app servicePort: http database: host: my-database-serviceApply your custom resource:kubectl apply -f nodejs_app_cr.yamlKopf will take care of creating the required resources (Deployment, Service, Ingress, and ConfigMap) based on the custom resource.ConclusionKopf simplifies the development and management of Kubernetes Operators, making it easier to automate complex application deployments. A few more points to consider when using an operator: Automated Application Management: Kubernetes Operators automate the deployment, scaling, and management of complex applications. They reduce manual intervention, minimize errors, and ensure consistency. Custom Resource Definitions (CRDs): Operators leverage Custom Resource Definitions (CRDs) to define application-specific resources. This allows you to extend Kubernetes to support custom applications and their lifecycles. Self-Healing: Operators can monitor the health of application components and perform automated recovery in case of failures Scalability: Operators can dynamically scale application components based on resource usage, ensuring optimal performance as workloads changes Enhanced Security: Operators can enforce security policies, secrets management, and access controls, contributing to a more secure application environment.In short, Kopf offer a simple way to setup K8s operators. While operators come with a learning curve and require development effort, the benefits in terms of automation, reliability, and scalability make them a great solution for managing applications in K8s." }, { "title": "Navigating HIPAA compliance on the Cloud with Lynis", "url": "/posts/lynis-audit/", "categories": "devops, cloud, security, compliance, hipaa, regulatory", "tags": "devops, cloud, security, compliance, hipaa, regulatory", "date": "2023-02-15 15:00:00 +0100", "snippet": "Healthcare organizations face a unique challenge when it comes to securing patient data in the cloud. Ensuring HIPAA (Health Insurance Portability and Accountability Act) compliance is a top priori...", "content": "Healthcare organizations face a unique challenge when it comes to securing patient data in the cloud. Ensuring HIPAA (Health Insurance Portability and Accountability Act) compliance is a top priority when it comes to healthcare related software. To make this process more approachable, we’re going to explore how to use Lynis, a fantastic open-source security auditing tool, to perform a HIPAA compliance audit in your AWS healthcare environment.What is Lynis?Lynis is a user-friendly auditing tool initially created for Linux systems. While it may not be an AWS or HIPAA-specific tool, it’s incredibly versatile and can be adapted to help you assess your cloud resources to match regulatory standards.Setting up LynisTo start your HIPAA compliance audit with Lynis, you’ll first need to have an AWS EC2 instance (or GCE if for example if you are using Google Cloud Provider). Once your instance is good to go, SSH into it and follow these steps:Step 1: Install LynisGet Lynis up and running on your EC2 instance by running the commands below:sudo apt-get updatesudo apt-get install lynisStep 2: Running Lynis with a HIPAA FocusLynis offers a helpful HIPAA profile that concentrates on checks directly related to HIPAA compliance. Other profiles also available are: ISO 27001 CIS Benchmarks PCI DSSTo run Lynis with the HIPAA profile, use this command:sudo lynis audit system --profile hipaaLynis will dive into your system based on the HIPAA profile, identifying potential areas where you might need to make improvements, and it’ll provide you with a detailed report.Understanding Lynis Results with HIPAA in MindAs the audit progresses, Lynis will provide insights into your HIPAA compliance. Here’s how to make sense of the results through a HIPAA lens: HIPAA-Focused Warnings and Suggestions: Look out for warnings and suggestions that are directly linked to HIPAA compliance. These might include advice on data encryption, access controls, and audit logging—important aspects of HIPAA. Getting a Sense of HIPAA Compliance: While Lynis doesn’t provide an official HIPAA compliance score, it does assign scores to different parts of the report, such as “Security Warnings” and “Suggestions.” These scores can give you a general idea of how your system is doing regarding security. Taking Action for HIPAA Compliance: Lynis will offer actionable items that are specifically aimed at achieving HIPAA compliance. These could include steps like configuring encrypted data transmission, setting up access controls, and ensuring your data logging and auditing settings are properly configured. Tracking Audit Trails: The Lynis report will tell you where to find log files that contain detailed information on each check Lynis performed. This includes checks that are crucial for HIPAA compliance. Example Report and HIPAA ActionsHere’s a simplified example of what a Lynis report might look like when it’s focused on HIPAA:[...]Hardening index: 72Tests performed: 260HIPAA-Related Suggestions: - Ensure data transmission is encrypted (HIPAA §164.312(e)(1)) - Implement access controls (HIPAA §164.312(a)(1)) - Set up audit logs (HIPAA §164.312(b))[...]In this example, Lynis suggests steps like enabling data transmission encryption, establishing access controls, and configuring audit logs.To tackle these recommendations and work towards HIPAA compliance in your cloud environment, follow the guidance Lynis provides or consult AWS documentation and the HIPAA guidelines for specific configuration steps.ConclusionLynis is a valuable tool for auditing HIPAA compliance in your environment. By performing regular audits, addressing findings, and sticking to best practices, you can maintain a secure and compliant infrastructure that protects sensitive patient data.If you reached here, thanks for reading, and feel free to provide any suggestions or feedback." }, { "title": "Building a multi-output Convolutional Neural Network with Keras", "url": "/posts/keras-multi-output/", "categories": "data science, machine learning", "tags": "data science, feature engineering, machine learning, python, pandas, scikit-learn, tensorflow, keras", "date": "2022-03-15 15:00:00 +0100", "snippet": "In this post, we will be exploring the Keras functional API in order to build a multi-output Deep Learning model. We will show how to train a single model that is capable of predicting three distin...", "content": "In this post, we will be exploring the Keras functional API in order to build a multi-output Deep Learning model. We will show how to train a single model that is capable of predicting three distinct outputs. By using the UTK Face dataset, which is composed of over 20 thousand pictures of people in uncontrolled environments, we will predict the age, gender and sex for each record presented in the dataset, reaching an accuracy of 91% for gender and 78% for race.The dataset The UTKFace dataset is a large dataset composed of over 20 thousand face images with their respectivce annotations of age, gender and ethnicity. The images are properly cropped into the face region, but display some variations in pose, illumination, resolution, etc.In order to retrieve the annotations of each record, we need to parse the filenames. Each record is stored in the following format: age_gender_race_date&amp;time.jpgWhere: age is an integer from 0 to 116 gender is an integer in which 0 represents male and 1 represents female race is an integer from 0 to 4, denoting white, black, asian, indian and others, respectively date and time, denoting when the picture was takenIf you want to know more about this dataset, please check their website.Let’s start by importing some libraries and creating our dictionary to help us on parsing the information from the dataset, along with some other information (dataset location, training split, width and height of the samples).import numpy as np import pandas as pdimport osimport globimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsdataset_folder_name = 'UTKFace'TRAIN_TEST_SPLIT = 0.7IM_WIDTH = IM_HEIGHT = 198dataset_dict = { 'race_id': { 0: 'white', 1: 'black', 2: 'asian', 3: 'indian', 4: 'others' }, 'gender_id': { 0: 'male', 1: 'female' }}dataset_dict['gender_alias'] = dict((g, i) for i, g in dataset_dict['gender_id'].items())dataset_dict['race_alias'] = dict((r, i) for i, r in dataset_dict['race_id'].items())Let’s also define a function to help us on extracting the data from our dataset. This function will be used to iterate over each file of the UTK dataset and return a Pandas Dataframe containing all the fields (age, gender and sex) of our records.def parse_dataset(dataset_path, ext='jpg'): \"\"\" Used to extract information about our dataset. It does iterate over all images and return a DataFrame with the data (age, gender and sex) of all files. \"\"\" def parse_info_from_file(path): \"\"\" Parse information from a single file \"\"\" try: filename = os.path.split(path)[1] filename = os.path.splitext(filename)[0] age, gender, race, _ = filename.split('_') return int(age), dataset_dict['gender_id'][int(gender)], dataset_dict['race_id'][int(race)] except Exception as ex: return None, None, None files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext)) records = [] for file in files: info = parse_info_from_file(file) records.append(info) df = pd.DataFrame(records) df['file'] = files df.columns = ['age', 'gender', 'race', 'file'] df = df.dropna() return dfdf = parse_dataset(dataset_folder_name)df.head()   age gender race file 0 30 male asian UTKFace/30_0_2_20170119183959989.jpg.chip.jpg 1 13 female others UTKFace/13_1_4_20170103200733438.jpg.chip.jpg 2 36 male white UTKFace/36_0_0_20170104204301875.jpg.chip.jpg 3 72 male black UTKFace/72_0_1_20170116205624331.jpg.chip.jpg 4 35 female white UTKFace/35_1_0_20170116201535811.jpg.chip.jpg 5 80 female white UTKFace/80_1_0_20170110182107291.jpg.chip.jpg 6 1 male asian UTKFace/1_0_2_20161219203236876.jpg.chip.jpg 7 25 female indian UTKFace/25_1_3_20170119171956657.jpg.chip.jpg 8 61 male indian UTKFace/61_0_3_20170109141653583.jpg.chip.jpg 9 32 male indian UTKFace/32_0_3_20170119200339548.jpg.chip.jpg Data visualizationAs an important step to understand not only the distribution of our dataset, but as well the predictions generated by our model, we need to perform some data visualization process on our dataset.We will start by defining a helper function to generate pie plots based on a given Pandas series:import plotly.graph_objects as godef plot_distribution(pd_series): labels = pd_series.value_counts().index.tolist() counts = pd_series.value_counts().values.tolist() pie_plot = go.Pie(labels=labels, values=counts, hole=.3) fig = go.Figure(data=[pie_plot]) fig.update_layout(title_text='Distribution for %s' % pd_series.name) fig.show()Race distributionLet’s start by plotting the race distribution with our predefined plot_distribution method.plot_distribution(df['race'])Having a quick glance at this plot, we can see that almost half of the samples are from the white race, so we can expect this group to have a great accuracy. Other races such as black, indian and asian also show a good number of samples, probably leading us to good accuracy numbers as well. The race ‘others’ (hispanics, latinos, etc) on the other side, show a small number of samples, being more likely to have a small accuracy.Gender distributionplot_distribution(df['gender'])For both male and female samples, we have quite a good balanced number of records, so we should have a great accuracy for both classes when using our model.Age distributionLet’s also plot how our age feature is distributed over the dataset by using a simple histogram with 20 bins.import plotly.express as pxfig = px.histogram(df, x=\"age\", nbins=20)fig.update_layout(title_text='Age distribution')fig.show()We can also display this same plot in a pie plot. Let’s group the age column into bins and then plot it with a pie chartbins = [0, 10, 20, 30, 40, 60, 80, np.inf]names = ['&lt;10', '10-20', '20-30', '30-40', '40-60', '60-80', '80+']age_binned = pd.cut(df['age'], bins, labels=names)plot_distribution(age_binned)We can observe that our dataset is mostly composed of individuals which age varies between 20 and 30 years, followed by individuals ranging from 30-40 years and then 40-60 years old. These groups represent around 70% of our dataset, so we can believe that we are going to have a good accuracy on predicting individuals in these ranges.We could also perform some multi-variate analysis on our dataset, but since the scope of this notebook is to demonstrate the usage of a multi-output model with Keras, we won’t be covering it.Data generatorIn order to input our data to our Keras multi-output model, we will create a helper object to work as a data generator for our dataset. This will be done by generating batches of data, which will be used to feed our multi-output model with both the images and their labels. This step is also done instead of just loading all the dataset into the memory at once, which could lead to an out of memory error.from keras.utils import to_categoricalfrom PIL import Imageclass UtkFaceDataGenerator(): \"\"\" Data generator for the UTKFace dataset. This class should be used when training our Keras multi-output model. \"\"\" def __init__(self, df): self.df = df def generate_split_indexes(self): p = np.random.permutation(len(self.df)) train_up_to = int(len(self.df) * TRAIN_TEST_SPLIT) train_idx = p[:train_up_to] test_idx = p[train_up_to:] train_up_to = int(train_up_to * TRAIN_TEST_SPLIT) train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:] # converts alias to id self.df['gender_id'] = self.df['gender'].map(lambda gender: dataset_dict['gender_alias'][gender]) self.df['race_id'] = self.df['race'].map(lambda race: dataset_dict['race_alias'][race]) self.max_age = self.df['age'].max() return train_idx, valid_idx, test_idx def preprocess_image(self, img_path): \"\"\" Used to perform some minor preprocessing on the image before inputting into the network. \"\"\" im = Image.open(img_path) im = im.resize((IM_WIDTH, IM_HEIGHT)) im = np.array(im) / 255.0 return im def generate_images(self, image_idx, is_training, batch_size=16): \"\"\" Used to generate a batch with images when training/testing/validating our Keras model. \"\"\" # arrays to store our batched data images, ages, races, genders = [], [], [], [] while True: for idx in image_idx: person = self.df.iloc[idx] age = person['age'] race = person['race_id'] gender = person['gender_id'] file = person['file'] im = self.preprocess_image(file) ages.append(age / self.max_age) races.append(to_categorical(race, len(dataset_dict['race_id']))) genders.append(to_categorical(gender, len(dataset_dict['gender_id']))) images.append(im) # yielding condition if len(images) &gt;= batch_size: yield np.array(images), [np.array(ages), np.array(races), np.array(genders)] images, ages, races, genders = [], [], [], [] if not is_training: break data_generator = UtkFaceDataGenerator(df)train_idx, valid_idx, test_idx = data_generator.generate_split_indexes()Building our modelIn this step, we will define our multi-output Keras model. Our model will be composed of three major branches, one for each available feature: age, gender and race.The default structure for our convolutional layers is based on a Conv2D layer with a ReLU activation, followed by a BatchNormalization layer, a MaxPooling and then finally a Dropout layer. Each of these layers is then followed by the final Dense layer. This step is repeated for each of the outputs we are trying to predict.These default layers are defined on the make_default_hidden_layers method, which will be reused on building each of the branches of our model. In the code below we will define our class that will be responsible for creating our multi-output model.from keras.models import Modelfrom keras.layers.normalization import BatchNormalizationfrom keras.layers.convolutional import Conv2Dfrom keras.layers.convolutional import MaxPooling2Dfrom keras.layers.core import Activationfrom keras.layers.core import Dropoutfrom keras.layers.core import Lambdafrom keras.layers.core import Densefrom keras.layers import Flattenfrom keras.layers import Inputimport tensorflow as tfclass UtkMultiOutputModel(): \"\"\" Used to generate our multi-output model. This CNN contains three branches, one for age, other for sex and another for race. Each branch contains a sequence of Convolutional Layers that is defined on the make_default_hidden_layers method. \"\"\" def make_default_hidden_layers(self, inputs): \"\"\" Used to generate a default set of hidden layers. The structure used in this network is defined as: Conv2D -&gt; BatchNormalization -&gt; Pooling -&gt; Dropout \"\"\" x = Conv2D(16, (3, 3), padding=\"same\")(inputs) x = Activation(\"relu\")(x) x = BatchNormalization(axis=-1)(x) x = MaxPooling2D(pool_size=(3, 3))(x) x = Dropout(0.25)(x) x = Conv2D(32, (3, 3), padding=\"same\")(x) x = Activation(\"relu\")(x) x = BatchNormalization(axis=-1)(x) x = MaxPooling2D(pool_size=(2, 2))(x) x = Dropout(0.25)(x) x = Conv2D(32, (3, 3), padding=\"same\")(x) x = Activation(\"relu\")(x) x = BatchNormalization(axis=-1)(x) x = MaxPooling2D(pool_size=(2, 2))(x) x = Dropout(0.25)(x) return x def build_race_branch(self, inputs, num_races): \"\"\" Used to build the race branch of our face recognition network. This branch is composed of three Conv -&gt; BN -&gt; Pool -&gt; Dropout blocks, followed by the Dense output layer. \"\"\" x = self.make_default_hidden_layers(inputs) x = Flatten()(x) x = Dense(128)(x) x = Activation(\"relu\")(x) x = BatchNormalization()(x) x = Dropout(0.5)(x) x = Dense(num_races)(x) x = Activation(\"softmax\", name=\"race_output\")(x) return x def build_gender_branch(self, inputs, num_genders=2): \"\"\" Used to build the gender branch of our face recognition network. This branch is composed of three Conv -&gt; BN -&gt; Pool -&gt; Dropout blocks, followed by the Dense output layer. \"\"\" x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs) x = self.make_default_hidden_layers(inputs) x = Flatten()(x) x = Dense(128)(x) x = Activation(\"relu\")(x) x = BatchNormalization()(x) x = Dropout(0.5)(x) x = Dense(num_genders)(x) x = Activation(\"sigmoid\", name=\"gender_output\")(x) return x def build_age_branch(self, inputs): \"\"\" Used to build the age branch of our face recognition network. This branch is composed of three Conv -&gt; BN -&gt; Pool -&gt; Dropout blocks, followed by the Dense output layer. \"\"\" x = self.make_default_hidden_layers(inputs) x = Flatten()(x) x = Dense(128)(x) x = Activation(\"relu\")(x) x = BatchNormalization()(x) x = Dropout(0.5)(x) x = Dense(1)(x) x = Activation(\"linear\", name=\"age_output\")(x) return x def assemble_full_model(self, width, height, num_races): \"\"\" Used to assemble our multi-output model CNN. \"\"\" input_shape = (height, width, 3) inputs = Input(shape=input_shape) age_branch = self.build_age_branch(inputs) race_branch = self.build_race_branch(inputs, num_races) gender_branch = self.build_gender_branch(inputs) model = Model(inputs=inputs, outputs = [age_branch, race_branch, gender_branch], name=\"face_net\") return model model = UtkMultiOutputModel().assemble_full_model(IM_WIDTH, IM_HEIGHT, num_races=len(dataset_dict['race_alias']))Let’s give a look into our model structure, to have a better understanding of what we are building. We can see from it that we have a single input, that in our case is the image we are feeding the CNN, which does decompose into three separated branches, each with their own Convolutions, followed by their respective Dense layers. Training our modelNow it’s time to train our multi-output model, once we have both the data ready to use and the model architecture defined. But before doing this step, we need to compile our model. For this task, we will use a learning rate of 0.0004 and an Adam optimizer, but you can be feel free to try with other hyperparameters. We will also use custom loss weights and a custom loss function for each feature.When building our optimizer, let’s use a decay based on the learning rate divided by the number of epochs, so we will slowly be decreasing our learning rate over the epochs.from keras.optimizers import Adaminit_lr = 1e-4epochs = 100opt = Adam(lr=init_lr, decay=init_lr / epochs)model.compile(optimizer=opt, loss={ 'age_output': 'mse', 'race_output': 'categorical_crossentropy', 'gender_output': 'binary_crossentropy'}, loss_weights={ 'age_output': 4., 'race_output': 1.5, 'gender_output': 0.1}, metrics={ 'age_output': 'mae', 'race_output': 'accuracy', 'gender_output': 'accuracy'})Now let’s train our model with a batch size of 32 for both valid and train sets. We will be using a ModelCheckpoint callback in order to save the model on disk at the end of each epoch.from keras.callbacks import ModelCheckpointbatch_size = 32valid_batch_size = 32train_gen = data_generator.generate_images(train_idx, is_training=True, batch_size=batch_size)valid_gen = data_generator.generate_images(valid_idx, is_training=True, batch_size=valid_batch_size)callbacks = [ ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')]history = model.fit_generator(train_gen, steps_per_epoch=len(train_idx)//batch_size, epochs=epochs, callbacks=callbacks, validation_data=valid_gen, validation_steps=len(valid_idx)//valid_batch_size)Once we have our model trained, let’s give a better look into how our model performed on both training and validation sets over the epochs:Race accuracyplt.clf()fig = go.Figure()fig.add_trace(go.Scatter( y=history.history['race_output_acc'], name='Train'))fig.add_trace(go.Scatter( y=history.history['val_race_output_acc'], name='Valid'))fig.update_layout(height=500, width=700, title='Accuracy for race feature', xaxis_title='Epoch', yaxis_title='Accuracy')fig.show()We can see that by epoch 50 our model stabilizes itself on the validation set, only increasing on the training one, with an accuracy of approximately 80%.Gender accuracyplt.clf()fig = go.Figure()fig.add_trace(go.Scatter( y=history.history['gender_output_acc'], name='Train'))fig.add_trace(go.Scatter( y=history.history['val_gender_output_acc'], name='Valid'))fig.update_layout(height=500, width=700, title='Accuracy for gender feature', xaxis_title='Epoch', yaxis_title='Accuracy')fig.show()Similarly to the race feature, we can see that our model is able to learn most of the patterns to properly predict the gender from a given individual by the 30th epoch, with an accuracy of approximately 90%.Age Mean Absolute Errorplt.clf()fig = go.Figure()fig.add_trace(go.Scattergl( y=history.history['age_output_mean_absolute_error'], name='Train'))fig.add_trace(go.Scattergl( y=history.history['val_age_output_mean_absolute_error'], name='Valid'))fig.update_layout(height=500, width=700, title='Mean Absolute Error for age feature', xaxis_title='Epoch', yaxis_title='Mean Absolute Error')fig.show()In the task of predicting the age feature, we can see that our model takes around 60 epochs to properly stabilize its learning process, with a mean absolute error of 0.09.Overall lossfig = go.Figure()fig.add_trace(go.Scattergl( y=history.history['loss'], name='Train'))fig.add_trace(go.Scattergl( y=history.history['val_loss'], name='Valid'))fig.update_layout(height=500, width=700, title='Overall loss', xaxis_title='Epoch', yaxis_title='Loss')fig.show()We can notice that by the epoch 50 our model starts to stabilize with a loss value of approximately 1.4. There is also a peak in the loss curve which does appear in the Mean Absolute Error for the age feature, which could explain the influence on the learning of the age feature on the overall loss.Evaluating our model on the test setIn order to assess how our model performs on the test set, let’s use our UTK data generator class, but this time using the test indexes. We will then call the predict_generator method from our trained model, which will output our the predictions for the test set.test_batch_size = 128test_generator = data_generator.generate_images(test_idx, is_training=False, batch_size=test_batch_size)age_pred, race_pred, gender_pred = model.predict_generator(test_generator, steps=len(test_idx)//test_batch_size)Let’s iterate one more time over all our test samples, in order to have their labels into a single list. We will also extract the arg max of each record, in order to retrieve the top predictions and ground truths.test_generator = data_generator.generate_images(test_idx, is_training=False, batch_size=test_batch_size)samples = 0images, age_true, race_true, gender_true = [], [], [], []for test_batch in test_generator: image = test_batch[0] labels = test_batch[1] images.extend(image) age_true.extend(labels[0]) race_true.extend(labels[1]) gender_true.extend(labels[2]) age_true = np.array(age_true)race_true = np.array(race_true)gender_true = np.array(gender_true)race_true, gender_true = race_true.argmax(axis=-1), gender_true.argmax(axis=-1)race_pred, gender_pred = race_pred.argmax(axis=-1), gender_pred.argmax(axis=-1)age_true = age_true * data_generator.max_ageage_pred = age_pred * data_generator.max_ageAnd finally, let’s print the classification reports for each feature on the test set.from sklearn.metrics import classification_reportcr_race = classification_report(race_true, race_pred, target_names=dataset_dict['race_alias'].keys())print(cr_race) precision recall f1-score support white 0.80 0.91 0.85 2994 black 0.86 0.82 0.84 1327 asian 0.86 0.79 0.83 1046 indian 0.74 0.74 0.74 1171 others 0.38 0.19 0.25 502 accuracy 0.80 7040 macro avg 0.73 0.69 0.70 7040weighted avg 0.78 0.80 0.78 7040From the report above, we can see that our model is really good at predicting asian and black individuals, with a precision of 0.86, followed by white people with 0.80 and indian with 0.74. The race ‘others’ shows a precision of only 0.38, but we need to take into consideration that this group is composed of different races and ethnicities along with a small number of samples, when compared to the other groups. The weighted accuracy for this classification task is 78%, showing that our classifier was able to properly learn patterns to distinguish different types of races.cr_gender = classification_report(gender_true, gender_pred, target_names=dataset_dict['gender_alias'].keys())print(cr_gender) precision recall f1-score support male 0.94 0.87 0.91 3735 female 0.87 0.94 0.90 3305 accuracy 0.90 7040 macro avg 0.90 0.91 0.90 7040weighted avg 0.91 0.90 0.90 7040From this report, we can notice that our model is really good at predicting the gender of a given individual, with a weighted accuracy of 91% for this task.from sklearn.metrics import r2_scoreprint('R2 score for age: ', r2_score(age_true, age_pred))R2 score for age: 0.5823979466456328Example of predictionsBelow we will plot some examples of the performed predictions generated by our model. We can clearly see that our model is really good at predicting gender, race and age, with some minor mistakes for the age feature. ReferencesUTK Face Dataset: http://aicip.eecs.utk.edu/wiki/UTKFaceKeras Multi-output documentation: https://keras.io/getting-started/functional-api-guide/SanjayaSubedi post on multi-output model: https://sanjayasubedi.com.np/deeplearning/multioutput-keras/PyImageSearch post on FashionNet: https://www.pyimagesearch.com/2018/06/04/keras-multiple-outputs-and-multiple-losses/Plotly: https://plot.ly/" }, { "title": "Keras 101: A simple Neural Network for House Pricing regression", "url": "/posts/keras-101-boston/", "categories": "data science", "tags": "data science, feature engineering, machine learning, python, pandas, scikit-learn, tensorflow, keras", "date": "2022-02-15 15:00:00 +0100", "snippet": "In this post, we will be covering some basics of data exploration and building a model with Keras in order to help us on predicting the selling price of a given house in the Boston (MA) area. As an...", "content": "In this post, we will be covering some basics of data exploration and building a model with Keras in order to help us on predicting the selling price of a given house in the Boston (MA) area. As an application of this model in the real world, you can think about being a real state agent looking for a tool to help you on your day-to-day duties, which for me, at least, sounds pretty good when compared to just gut-estimation.For this exercise, we will be using the Plotly library instead of the good ol’ fashioned matplotlib, due to having more interactive plots, which for sure help in understanding the data. We will also use the Scikit-Learn and Keras for building the models, Pandas library to manipulate our data and the SHAP library to generate explanations for our trained model.Importing the datasetIn this example, we wil be using the sklearn.datasets module, which contains the Boston dataset. You could also use the keras.datasets module, but this one does not contain the labels of the features, so we decided to use scikits one. Let’s also convert it to a Pandas DataFrame and print it’s head.from sklearn.datasets import load_bostonimport pandas as pdboston_dataset = load_boston()df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)df['MEDV'] = boston_dataset.targetdf.head(n=10)This should output the following dataframe:   CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18 2.31 0 0.538 6.575 65.2 4.09 1 296 15.3 396.9 4.98 24 1 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.9 9.14 21.6 2 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 34.7 3 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 33.4 4 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.9 5.33 36.2 Exploratory Data AnalysisMaking yourself comfortable and familiar with your dataset is a fundamental step to help you comprehend your data and draw better conclusions and explanations from your results.Initially, let’s plot a few box plots, which will help us to better visualize anomalies and/or outliers in data distribution. If you are confused about what is a box plot and how it can help us to better visualizate the distribution of our data, here is a brief description from Ross (1977): In descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.from plotly.subplots import make_subplotsimport plotly.graph_objects as goimport mathtotal_items = len(df.columns)items_per_row = 3total_rows = math.ceil(total_items / items_per_row)fig = make_subplots(rows=total_rows, cols=items_per_row)cur_row = 1cur_col = 1for index, column in enumerate(df.columns): fig.add_trace(go.Box(y=df[column], name=column), row=cur_row, col=cur_col) if cur_col % items_per_row == 0: cur_col = 1 cur_row = cur_row + 1 else: cur_col = cur_col + 1 fig.update_layout(height=1000, width=550, showlegend=False)fig.show()You can hover on the subplots below to see the properties of each box plot.These results do corroborate our initial assumptions about having outliers in some columns. Let’s also plot some scatter plots for each feature and the target variable, as well as their intercept lines:from plotly.subplots import make_subplotsimport plotly.graph_objects as goimport mathimport numpy as nptotal_items = len(df.columns)items_per_row = 3total_rows = math.ceil(total_items / items_per_row)fig = make_subplots(rows=total_rows, cols=items_per_row, subplot_titles=df.columns)cur_row = 1cur_col = 1for index, column in enumerate(df.columns): fig.add_trace(go.Scattergl(x=df[column], y=df['MEDV'], mode=\"markers\", marker=dict(size=3)), row=cur_row, col=cur_col) intercept = np.poly1d(np.polyfit(df[column], df['MEDV'], 1))(np.unique(df[column])) fig.add_trace(go.Scatter(x=np.unique(df[column]), y=intercept, line=dict(color='red', width=1)), row=cur_row, col=cur_col) if cur_col % items_per_row == 0: cur_col = 1 cur_row = cur_row + 1 else: cur_col = cur_col + 1 fig.update_layout(height=1000, width=550, showlegend=False)fig.show() From this initial data exploration, we can have two major conclusions: There is a strong linear correlation between the RM (average number of rooms per dwelling) and LSTAT (% lower status of the population) with the target variable, being the RM a positive and the LSTAT a negative correlation. There are some records containing outliers, which we could preprocess in order to input our model with more normalized data.Data preprocessingBefore we proceed into any data preprocessing, it’s important to split our data into training and test sets. We should not apply any kind of preprocessing into our data without taking into account that we should not leak information from our test set. For this step, we can use the train_test_split method from scikit-learn. In this case, we will use a split of 70% of the data for training and 30% for testing. We also set a random_state seed, in order to allow reprocibility.from sklearn.model_selection import train_test_splitX = df.loc[:, df.columns != 'MEDV']y = df.loc[:, df.columns == 'MEDV']X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)In order to provide a standardized input to our neural network, we need the perform the normalization of our dataset. This can be seen as an step to reduce the differences in scale that may arise from the existent features. We perform this normalization by subtracting the mean from our data and dividing it by the standard deviation. One more time, this normalization should only be performed by using the mean and standard deviation from the training set, in order to avoid any information leak from the test set.mean = X_train.mean(axis=0)std = X_train.std(axis=0)X_train = (X_train - mean) / stdX_test = (X_test - mean) / stdBuild our modelDue to the small amount of presented data in this dataset, we must be careful to not create an overly complex model, which could lead to overfitting our data. For this, we are going to adopt an architecture based on two Dense layers, the first with 128 and the second with 64 neurons, both using a ReLU activation function. A dense layer with a linear activation will be used as output layer.In order to allow us to know if our model is properly learning, we will use a mean squared error loss function and to report the performance of it we will adopt the mean average error metric.By using the summary method from Keras, we can see that we have a total of 10,113 parameters, which is acceptable for us.from keras.models import Sequentialfrom keras.layers import Densemodel = Sequential()model.add(Dense(128, input_shape=(13, ), activation='relu', name='dense_1'))model.add(Dense(64, activation='relu', name='dense_2'))model.add(Dense(1, activation='linear', name='dense_output'))model.compile(optimizer='adam', loss='mse', metrics=['mae'])model.summary()Train our modelThis step is pretty straightforward: fit our model with both our features and their labels, for a total amount of 100 epochs, separating 5% of the samples (18 records) as validation set.history = model.fit(X_train, y_train, epochs=100, validation_split=0.05)By plotting both loss and mean average error, we can see that our model was capable of learning patterns in our data without overfitting taking place (as shown by the validation set curves):fig = go.Figure()fig.add_trace(go.Scattergl(y=history.history['loss'], name='Train'))fig.add_trace(go.Scattergl(y=history.history['val_loss'], name='Valid'))fig.update_layout(height=500, width=700, xaxis_title='Epoch', yaxis_title='Loss')fig.show()fig = go.Figure()fig.add_trace(go.Scattergl(y=history.history['mean_absolute_error'], name='Train'))fig.add_trace(go.Scattergl(y=history.history['val_mean_absolute_error'], name='Valid'))fig.update_layout(height=500, width=700, xaxis_title='Epoch', yaxis_title='Mean Absolute Error')fig.show()Evaluate our modelmse_nn, mae_nn = model.evaluate(X_test, y_test)print('Mean squared error on test data: ', mse_nn)print('Mean absolute error on test data: ', mae_nn)Output:152/152 [==============================] - 0s 60us/stepMean squared error on test data: 17.429732523466413Mean absolute error on test data: 2.6727954964888725Comparison with traditional approachesFirst let’s try with a simple algorithm, the Linear Regression:lr_model = LinearRegression()lr_model.fit(X_train, y_train)y_pred_lr = lr_model.predict(X_test)mse_lr = mean_squared_error(y_test, y_pred_lr)mae_lr = mean_absolute_error(y_test, y_pred_lr)print('Mean squared error on test data: ', mse_lr)print('Mean absolute error on test data: ', mae_lr)Mean squared error on test data: 28.40585481050824Mean absolute error on test data: 3.6913626771162575And now with a Decision Tree:tree = DecisionTreeRegressor()tree.fit(X_train, y_train)y_pred_tree = tree.predict(X_test)mse_dt = mean_squared_error(y_test, y_pred_tree)mae_dt = mean_absolute_error(y_test, y_pred_tree)print('Mean squared error on test data: ', mse_dt)print('Mean absolute error on test data: ', mae_dt)Mean squared error on test data: 17.830657894736845Mean absolute error on test data: 2.755263157894737Opening the Black Box (a.k.a. Explaining our Model)Sometimes just a good result is enough for most of the people, but there are scenarios where we need to explain what are the major components used by our model to perform its prediction. For this task, we can rely on the SHAP library, which easily allows us to create a summary of our features and its impact on the model output. I won’t dive deep into the details of SHAP, but if you are intered on it, you can check their github page or even give a look at its paper].import shapshap.initjs()explainer = shap.DeepExplainer(model, X_train[:100].values)shap_values = explainer.shap_values(X_test[:100].values)shap.summary_plot(shap_values, X_test, plot_type='bar') From this simple plot, we can see that the major features that have an impact on the model output are: LSTAT: % lower status of the population RM: average number of rooms per dwelling RAD: index of accessibility to radial highways DIS: weighted distances to five Boston employment centres NOX: nitric oxides concentration (parts per 10 million) - this may more likely be correlated with greenness of the area CRIM: per capita crime rate by townFrom this plot, we can clearly corroborate our initial EDA analysis in which we point out the LSTAT and RM features as having a high correlation with the model outcome.ConclusionsIn this post, we have showed that by using a Neural Network, we can easily outperform traditional Machine Learning methods by a good margin. We also show that, even when using a more complex model, when compared to other techniques, we can still explain the outcomes of our model by using the SHAP library.Furthermore, we need to have in mind that the explored dataset can be somehow outdated, and some feature engineering (such as correcting prices for inflaction) could be performed in order to better reflect current scenarios.The Jupyter notebook for this post can be found here.ReferencesBoston Dataset: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.htmlPlotly: https://plot.ly/python/ScikitLearn: https://scikit-learn.org/stable/Keras: https://keras.io/Pandas: https://pandas.pydata.org/SHAP Project Page: https://github.com/slundberg/shapSHAP Paper: https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdfIntroduction to probability and statistics for engineers and scientists. https://www.amazon.com.br/dp/B007ZBZN9U/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1" }, { "title": "Elucidating Machine Learning models with SHAP values", "url": "/posts/shap/", "categories": "data science", "tags": "data science, feature engineering, machine learning, python, pandas, scikit-learn, tensorflow, keras", "date": "2022-01-15 15:00:00 +0100", "snippet": "The problemDuring data analysis, we may only look at metrics such as accuracy/precision/recall and worry about not overfitting our data. But are those the only ones that matter?When it comes to rea...", "content": "The problemDuring data analysis, we may only look at metrics such as accuracy/precision/recall and worry about not overfitting our data. But are those the only ones that matter?When it comes to real world scenarios, when we need not only to provide a model with a good prediction rate, but as well the explanations behind the decisions taken by our model, it is quite a difficult task: if we go with simpler and easier to explain models, we leave some performance on the table; if we just aim for performance, we go for highly complex models, but at the cost of the explainability.Sounds quite a tricky tradeoff, doesn’t it? We also need to keep in mind, that sometimes the explainability of our decision-making process need to be clear, such as the ones proposed in the GDPR (right of explanation section).A possible solutionBut, if we can still adopt complex models while keeping the explainability factor? For this we can rely on a technique called SHAPley values, which is heavily based on the Game Theory.In a short explanation, we can describe the SHAPley values as a measurement of the importance of each feature into our models final prediction. This definition, of course, is a really reduced one, so if you are interested in a more detailed one, there are two great resources to look for [1] [2] .A hands-on exampleAt BraSCCH (Brazilian Smart Care for Child Health), we most of the time deal with training Machine Learning models that can help us on the task of identifying neonatal death.For this task, we use a dataset that contains several features related to a given newborn, such as: mother’s age, the newborn weight, vital signs (such as breathing, muscular tonus, appearance, etc). What if we wanted to know, even when using a complex model, such as a Extreme Gradient Boosting, or a Neural Network, which are the features that play a major role in the prediction result?For this task, we will be used the concept of SHAPley values, implemented on the SHAP library, which is primarily developed in Python and can be installed directly from pip (pip install shap and that’s all).Displaying a summary plotIn order to have a clear vision of the features and their impact on our model, we can use a summary plot, which sorts the features by their SHAP values, representing their influence on the model output.By displaying our summary plot, we can see that the features that play a major role in the model decision are: Newborn weight APGAR 5 (vital signs of the newborn at the 5th minute of life), if there was any identified anomaly APGAR 1 (similar to APGAR 5, but at the 1st minute) Weeks of pregnancy and number of prenatal consultations.Just an example to explain one of the listed features, we can observe that for the feature weight, as it goes higher in value, smaller is the SHAP value for it, thus indicating that newborns with a large weight have a smaller impact on the classifier output, while that newborns with a lower weight do contribute largely to an increased SHAP value and consequently to the model final prediction.We can also see this same summary plot in a more simplistic manner, by specifying the plot_type parameter as bar.This plot conveys pretty much a simpler explanation about our model, just displaying the features and their averaged impacts.The dependence plotAnother highly used plot when trying to understand the correlation between the features and the model outcomes is the dependence plot. This plot allows us to see how a given feature impacts the SHAP value along with the distribution of another feature (commonly referred as interaction feature).In this plot, we display the mother’s education in years (x axis), correlated with the respective SHAP values (y axis) and the interaction with the feature “weeks of pregnancy” (color bar at right).We can clearly notice that, the higher the education of the mother, less likely it is for the newborn to die. This may likely happen due to socio-economical factors, since more educated mothers are more likely to have a higher income and thus better living conditions.Related to this, we can see how the weeks of pregnancy is distributed over the given education category groups. We can notice that for mothers with a high education, the major factor of dying is most likely to be related to a low number of weeks of pregnancy, while that this pattern is not displayed for the other groups.ConclusionsIn this post, we briefly went through one of the tools used to explain Machine Learning models, but there are several others, such as Lime and Interpret. I recommend everyone to play a little bit with those, to have a better grasp of how these tools can better help us to understand not only about our model outcomes, but also about our given problem." }, { "title": "Enhancing Categorical Features with Entity Embeddings", "url": "/posts/entity-embeddings/", "categories": "data science", "tags": "data science, feature engineering, machine learning, python, pandas, scikit-learn, tensorflow, keras", "date": "2021-12-20 15:00:00 +0100", "snippet": "Let’s talk about selling beers.Let’s suppose you are the owner of a pub, and you would like to predict how many beers your establishment is going to sell on a given day based on two variables: the...", "content": "Let’s talk about selling beers.Let’s suppose you are the owner of a pub, and you would like to predict how many beers your establishment is going to sell on a given day based on two variables: the day of the week the current weather We can probably hypothesize that weekends and warm days are going to sell more beers when compared to the beginning of the week and cold days, right? Let’s see if this hypothesis holds to be trueIn face of this problem, we usually would start by encoding our categorical data (in this example, the day of the week and the weather) into dummy variables, in order to provide an input to our classifier without any kind of relationship between the existing categorical values.Our data would look like something below, for the day of week feature (you can imagine something similar for the weather feature):But does this really makes sense, to treat each categorical value as being completely different from one another, such as when using One-Hot-Encoding? Or even better:Could we make usage of some sort of technique to “learn” the similarities between each possible value and their outcome?Entity Embeddings to the rescueWith this given scenario in mind, we can then proceed to the adoption of a technique popularly known in the NLP (Natural Language Processing) field as Entity Embeddings, which allows us to map a given feature set into a new one with a smaller number of dimensions. In our case, it will also allow us to extract meaningful information from our categorical data.The usage of Entity Embeddings is based on the process of training of a Neural Network with the categorical data, with the ultimate purpose to retrieve the weights of the Embedding layers, allowing us to have a more significant input when compared to a single One-Hot-Encoding approach. By adopting Entity Embeddings we also are able to mitigate two major problems: No need to have a domain expert, once we’re capable to train a Neural Network that can efficiently learn patterns and relationships between the values of a same categorical feature, thus removing the step of feature engineering (such as manually giving weights to each day of the week or kind of weather); Shrinkage on computing resources, once we’re not longer just directly encoding our possible categorical values with One-Hot-Encoding, which can represent a huge resource usage: Let’s just suppose you have a categorical feature with 10 thousand possible unique values. This would translate into a feature vector with the same amount of empty positions just to represent a given value.The definition of Entity EmbeddingIn short words, the Embedding layer is pretty much a Neural Network layer that groups, in a N-dimensional space, categorical values with similar output value. This spatial representation allows us to obtain intrinsic properties of each categorical value, which can be later on used as a replacement to our old dummy encoded variables. If we think about it in a more simple manner, it would mean that days of the week that have a similar output (in our case, number of sold beers), would be close to each other. If you don’t get it, a picture can help:Here we can see that we have four major groups: Group 1, with Monday and Tuesday, possibly related to a low amount of sold beers, due to being the start of the week Group 2, with Wednesday and Thursday, with some distance from group 1 Group 3, with Friday and Saturday, relatively close to group 2, indicating that they show more similarity than when compared with group 1 Group 4, with Sunday, without many similarities to the other groupsThis simple example can show us that the embedding layers can learn information from the real world, such as the most common days for going out and drinking. Pretty cool, isn’t it?Putting it together with KerasFirst of all, we need to know that for using an embedding layer, we must specify the number of dimensions we would like to be used for that given embedding. This, as you can notice, is a hyperparameter, and should be tested and experimented case by case. But as a rule of thumb, you can adopt the number of dimensions as equal to the square root of the number of unique values for the category. So in our case, our representation for the day of the week would have instead of seven different positions, only three (we round up in our case, since the square root of 7 is 2.64). Below we give an example for both mentioned features, as well as add some hidden layers, in order to have more parameters in our model to capture minor data nuances.# Embedding layer for the 'Day of Week' featuren_unique_day = df['Day'].nunique()n_dim_day = int(sqrt(n_unique_day))input_week = Input(shape=(1, ))output_week = Embedding(input_dim=n_unique_day, output_dim=n_dim_day, name=\"day\")(input_week)output_week = Reshape(target_shape=(n_dim_day, ))(output_week)# Embedding layer for the 'Weather' featuren_unique_weather = df['Weather'].nunique()n_dim_weather = int(sqrt(n_unique_weather))input_weather = Input(shape=(1, ))output_weather = Embedding(input_dim=n_unique_weather, output_dim=n_dim_weather, name=\"weather\")(input_weather)output_weather = Reshape(target_shape=(n_dim_weather,))(output_weather)input_layers = [input_week, input_weather]output_layers = [output_week, output_weather]model = Concatenate()(output_layers)# Add a few hidden layersmodel = Dense(200, kernel_initializer=\"uniform\")(model)model = Activation('relu')(model)model = Dense(100, kernel_initializer=\"uniform\")(model)model = Activation('relu')(model)# And finally our output layermodel = Dense(1)(model)model = Activation('sigmoid')(model)# Put it all together and compile the modelmodel = KerasModel(inputs=input_layers, outputs=model)model.summary()opt = SGD(lr=0.05)model.compile(loss='mse', optimizer=opt, metrics=['mse'])Graphically our Neural Network would have the following representation:ResultsThat’s it. We can see that our architecture is composed from a Input layer for each of the categorical values, followed by our Embedding layers, then a Reshape layer and then all put together. Lastly, we add some hidden layers to capture minor nuances of our data. Training our network for 200 epochs with a learning rate of 0.05, we can see some pretty good results for loss and mean squared error:ConclusionsIn this example it may sound silly, but we can again think about our scenario of 10 thousand unique values. The difference between a feature vector with 10 thousand positions (by using One-Hot-Encoding) and another with only 100 (measured by the rule of thumb, when using entity embeddings) is enormous. This is the difference for only a single record for a single feature. You can imagine how complex this becomes with a real world dataset.If you reached until this point without any doubts, congratulations! But if you do have any kind of questions, suggestions or complains, feel free to reach meSource CodeIf you want to check the full source code for this example, you can find it in my GitHub" } ]
