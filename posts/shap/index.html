<!doctype html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Elucidating Machine Learning models with SHAP values" /><meta property="og:locale" content="en" /><meta name="description" content="The problem During data analysis, we may only look at metrics such as accuracy/precision/recall and worry about not overfitting our data. But are those the only ones that matter? When it comes to real world scenarios, when we need not only to provide a model with a good prediction rate, but as well the explanations behind the decisions taken by our model, it is quite a difficult task: if we go with simpler and easier to explain models, we leave some performance on the table; if we just aim for performance, we go for highly complex models, but at the cost of the explainability. Sounds quite a tricky tradeoff, doesn’t it? We also need to keep in mind, that sometimes the explainability of our decision-making process need to be clear, such as the ones proposed in the GDPR (right of explanation section). A possible solution But, if we can still adopt complex models while keeping the explainability factor? For this we can rely on a technique called SHAPley values, which is heavily based on the Game Theory. In a short explanation, we can describe the SHAPley values as a measurement of the importance of each feature into our models final prediction. This definition, of course, is a really reduced one, so if you are interested in a more detailed one, there are two great resources to look for [1] [2] . A hands-on example At BraSCCH (Brazilian Smart Care for Child Health), we most of the time deal with training Machine Learning models that can help us on the task of identifying neonatal death. For this task, we use a dataset that contains several features related to a given newborn, such as: mother’s age, the newborn weight, vital signs (such as breathing, muscular tonus, appearance, etc). What if we wanted to know, even when using a complex model, such as a Extreme Gradient Boosting, or a Neural Network, which are the features that play a major role in the prediction result? For this task, we will be used the concept of SHAPley values, implemented on the SHAP library, which is primarily developed in Python and can be installed directly from pip (pip install shap and that’s all). Displaying a summary plot In order to have a clear vision of the features and their impact on our model, we can use a summary plot, which sorts the features by their SHAP values, representing their influence on the model output. By displaying our summary plot, we can see that the features that play a major role in the model decision are: Newborn weight APGAR 5 (vital signs of the newborn at the 5th minute of life), if there was any identified anomaly APGAR 1 (similar to APGAR 5, but at the 1st minute) Weeks of pregnancy and number of prenatal consultations. Just an example to explain one of the listed features, we can observe that for the feature weight, as it goes higher in value, smaller is the SHAP value for it, thus indicating that newborns with a large weight have a smaller impact on the classifier output, while that newborns with a lower weight do contribute largely to an increased SHAP value and consequently to the model final prediction. We can also see this same summary plot in a more simplistic manner, by specifying the plot_type parameter as bar. This plot conveys pretty much a simpler explanation about our model, just displaying the features and their averaged impacts. The dependence plot Another highly used plot when trying to understand the correlation between the features and the model outcomes is the dependence plot. This plot allows us to see how a given feature impacts the SHAP value along with the distribution of another feature (commonly referred as interaction feature). In this plot, we display the mother’s education in years (x axis), correlated with the respective SHAP values (y axis) and the interaction with the feature “weeks of pregnancy” (color bar at right). We can clearly notice that, the higher the education of the mother, less likely it is for the newborn to die. This may likely happen due to socio-economical factors, since more educated mothers are more likely to have a higher income and thus better living conditions. Related to this, we can see how the weeks of pregnancy is distributed over the given education category groups. We can notice that for mothers with a high education, the major factor of dying is most likely to be related to a low number of weeks of pregnancy, while that this pattern is not displayed for the other groups. Conclusions In this post, we briefly went through one of the tools used to explain Machine Learning models, but there are several others, such as Lime and Interpret. I recommend everyone to play a little bit with those, to have a better grasp of how these tools can better help us to understand not only about our model outcomes, but also about our given problem." /><meta property="og:description" content="The problem During data analysis, we may only look at metrics such as accuracy/precision/recall and worry about not overfitting our data. But are those the only ones that matter? When it comes to real world scenarios, when we need not only to provide a model with a good prediction rate, but as well the explanations behind the decisions taken by our model, it is quite a difficult task: if we go with simpler and easier to explain models, we leave some performance on the table; if we just aim for performance, we go for highly complex models, but at the cost of the explainability. Sounds quite a tricky tradeoff, doesn’t it? We also need to keep in mind, that sometimes the explainability of our decision-making process need to be clear, such as the ones proposed in the GDPR (right of explanation section). A possible solution But, if we can still adopt complex models while keeping the explainability factor? For this we can rely on a technique called SHAPley values, which is heavily based on the Game Theory. In a short explanation, we can describe the SHAPley values as a measurement of the importance of each feature into our models final prediction. This definition, of course, is a really reduced one, so if you are interested in a more detailed one, there are two great resources to look for [1] [2] . A hands-on example At BraSCCH (Brazilian Smart Care for Child Health), we most of the time deal with training Machine Learning models that can help us on the task of identifying neonatal death. For this task, we use a dataset that contains several features related to a given newborn, such as: mother’s age, the newborn weight, vital signs (such as breathing, muscular tonus, appearance, etc). What if we wanted to know, even when using a complex model, such as a Extreme Gradient Boosting, or a Neural Network, which are the features that play a major role in the prediction result? For this task, we will be used the concept of SHAPley values, implemented on the SHAP library, which is primarily developed in Python and can be installed directly from pip (pip install shap and that’s all). Displaying a summary plot In order to have a clear vision of the features and their impact on our model, we can use a summary plot, which sorts the features by their SHAP values, representing their influence on the model output. By displaying our summary plot, we can see that the features that play a major role in the model decision are: Newborn weight APGAR 5 (vital signs of the newborn at the 5th minute of life), if there was any identified anomaly APGAR 1 (similar to APGAR 5, but at the 1st minute) Weeks of pregnancy and number of prenatal consultations. Just an example to explain one of the listed features, we can observe that for the feature weight, as it goes higher in value, smaller is the SHAP value for it, thus indicating that newborns with a large weight have a smaller impact on the classifier output, while that newborns with a lower weight do contribute largely to an increased SHAP value and consequently to the model final prediction. We can also see this same summary plot in a more simplistic manner, by specifying the plot_type parameter as bar. This plot conveys pretty much a simpler explanation about our model, just displaying the features and their averaged impacts. The dependence plot Another highly used plot when trying to understand the correlation between the features and the model outcomes is the dependence plot. This plot allows us to see how a given feature impacts the SHAP value along with the distribution of another feature (commonly referred as interaction feature). In this plot, we display the mother’s education in years (x axis), correlated with the respective SHAP values (y axis) and the interaction with the feature “weeks of pregnancy” (color bar at right). We can clearly notice that, the higher the education of the mother, less likely it is for the newborn to die. This may likely happen due to socio-economical factors, since more educated mothers are more likely to have a higher income and thus better living conditions. Related to this, we can see how the weeks of pregnancy is distributed over the given education category groups. We can notice that for mothers with a high education, the major factor of dying is most likely to be related to a low number of weeks of pregnancy, while that this pattern is not displayed for the other groups. Conclusions In this post, we briefly went through one of the tools used to explain Machine Learning models, but there are several others, such as Lime and Interpret. I recommend everyone to play a little bit with those, to have a better grasp of how these tools can better help us to understand not only about our model outcomes, but also about our given problem." /><link rel="canonical" href="https://bressan.xyz/posts/shap/" /><meta property="og:url" content="https://bressan.xyz/posts/shap/" /><meta property="og:site_name" content="Rodrigo Bressan" /><meta property="og:image" content="https://bressan.xyz/assets/img/posts/02-shap/banner.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-15T15:00:00+01:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://bressan.xyz/assets/img/posts/02-shap/banner.png" /><meta property="twitter:title" content="Elucidating Machine Learning models with SHAP values" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-01-15T15:00:00+01:00","datePublished":"2022-01-15T15:00:00+01:00","description":"The problem During data analysis, we may only look at metrics such as accuracy/precision/recall and worry about not overfitting our data. But are those the only ones that matter? When it comes to real world scenarios, when we need not only to provide a model with a good prediction rate, but as well the explanations behind the decisions taken by our model, it is quite a difficult task: if we go with simpler and easier to explain models, we leave some performance on the table; if we just aim for performance, we go for highly complex models, but at the cost of the explainability. Sounds quite a tricky tradeoff, doesn’t it? We also need to keep in mind, that sometimes the explainability of our decision-making process need to be clear, such as the ones proposed in the GDPR (right of explanation section). A possible solution But, if we can still adopt complex models while keeping the explainability factor? For this we can rely on a technique called SHAPley values, which is heavily based on the Game Theory. In a short explanation, we can describe the SHAPley values as a measurement of the importance of each feature into our models final prediction. This definition, of course, is a really reduced one, so if you are interested in a more detailed one, there are two great resources to look for [1] [2] . A hands-on example At BraSCCH (Brazilian Smart Care for Child Health), we most of the time deal with training Machine Learning models that can help us on the task of identifying neonatal death. For this task, we use a dataset that contains several features related to a given newborn, such as: mother’s age, the newborn weight, vital signs (such as breathing, muscular tonus, appearance, etc). What if we wanted to know, even when using a complex model, such as a Extreme Gradient Boosting, or a Neural Network, which are the features that play a major role in the prediction result? For this task, we will be used the concept of SHAPley values, implemented on the SHAP library, which is primarily developed in Python and can be installed directly from pip (pip install shap and that’s all). Displaying a summary plot In order to have a clear vision of the features and their impact on our model, we can use a summary plot, which sorts the features by their SHAP values, representing their influence on the model output. By displaying our summary plot, we can see that the features that play a major role in the model decision are: Newborn weight APGAR 5 (vital signs of the newborn at the 5th minute of life), if there was any identified anomaly APGAR 1 (similar to APGAR 5, but at the 1st minute) Weeks of pregnancy and number of prenatal consultations. Just an example to explain one of the listed features, we can observe that for the feature weight, as it goes higher in value, smaller is the SHAP value for it, thus indicating that newborns with a large weight have a smaller impact on the classifier output, while that newborns with a lower weight do contribute largely to an increased SHAP value and consequently to the model final prediction. We can also see this same summary plot in a more simplistic manner, by specifying the plot_type parameter as bar. This plot conveys pretty much a simpler explanation about our model, just displaying the features and their averaged impacts. The dependence plot Another highly used plot when trying to understand the correlation between the features and the model outcomes is the dependence plot. This plot allows us to see how a given feature impacts the SHAP value along with the distribution of another feature (commonly referred as interaction feature). In this plot, we display the mother’s education in years (x axis), correlated with the respective SHAP values (y axis) and the interaction with the feature “weeks of pregnancy” (color bar at right). We can clearly notice that, the higher the education of the mother, less likely it is for the newborn to die. This may likely happen due to socio-economical factors, since more educated mothers are more likely to have a higher income and thus better living conditions. Related to this, we can see how the weeks of pregnancy is distributed over the given education category groups. We can notice that for mothers with a high education, the major factor of dying is most likely to be related to a low number of weeks of pregnancy, while that this pattern is not displayed for the other groups. Conclusions In this post, we briefly went through one of the tools used to explain Machine Learning models, but there are several others, such as Lime and Interpret. I recommend everyone to play a little bit with those, to have a better grasp of how these tools can better help us to understand not only about our model outcomes, but also about our given problem.","headline":"Elucidating Machine Learning models with SHAP values","image":"https://bressan.xyz/assets/img/posts/02-shap/banner.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://bressan.xyz/posts/shap/"},"url":"https://bressan.xyz/posts/shap/"}</script><title>Elucidating Machine Learning models with SHAP values | Rodrigo Bressan</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Rodrigo Bressan"><meta name="application-name" content="Rodrigo Bressan"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.1/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"> <img src="/assets/img/me.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"> </a><h1 class="site-title"> <a href="/">Rodrigo Bressan</a></h1><p class="site-subtitle fst-italic mb-0">Software, Healthcare and Improving People's lives</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a><li class="nav-item"> <a href="/contact/" class="nav-link"> <i class="fa-fw fas fa-address-card"></i> <span>CONTACT</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fas fa-code"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/resume/" class="nav-link"> <i class="fa-fw fas fa-file"></i> <span>RESUME</span> </a><li class="nav-item"> <a href="/all-posts/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ALL POSTS</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <a href="https://github.com/rodrigobressan" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://www.linkedin.com/in/rbressan/" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container px-xxl-5"><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Elucidating Machine Learning models with SHAP values</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4" ><article class="px-1"><header><h1 data-toc-skip>Elucidating Machine Learning models with SHAP values</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1642255200" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jan 15, 2022 </time> </span><div class="mt-3 mb-3"> <a href="/assets/img/posts/02-shap/banner.png" class="popup img-link preview-img shimmer"><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1200 630'%3E%3C/svg%3E" data-src="/assets/img/posts/02-shap/banner.png" alt="Preview Image" width="1200" height="630" class="lazyload" data-proofer-ignore></a></div><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/rodrigobressan">Rodrigo Bressan</a> </em> </span> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="830 words" > <em>4 min</em> read</span></div></div></header><div class="content"><h2 id="the-problem"><span class="me-2">The problem</span><a href="#the-problem" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>During data analysis, we may only look at metrics such as accuracy/precision/recall and worry about not overfitting our data. <b>But are those the only ones that matter?</b></p><p>When it comes to real world scenarios, when we need not only to provide a model with a good prediction rate, but as well the explanations behind the decisions taken by our model, it is quite a difficult task:</p><ul><li>if we go with simpler and easier to explain models, we leave some performance on the table;<li>if we just aim for performance, we go for highly complex models, but at the cost of the explainability.</ul><p>Sounds quite a tricky tradeoff, doesn’t it? We also need to keep in mind, that sometimes the explainability of our decision-making process need to be clear, such as the ones proposed in the GDPR (right of explanation section).</p><h2 id="a-possible-solution"><span class="me-2">A possible solution</span><a href="#a-possible-solution" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>But, if we can still adopt complex models while keeping the explainability factor? For this we can rely on a technique called SHAPley values, which is heavily based on the Game Theory.</p><p>In a short explanation, we can describe the SHAPley values as a measurement of the importance of each feature into our models final prediction. This definition, of course, is a really reduced one, so if you are interested in a more detailed one, there are two great resources to look for <a href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions">[1]</a> <a href="https://arxiv.org/abs/1905.04610">[2]</a> .</p><h2 id="a-hands-on-example"><span class="me-2">A hands-on example</span><a href="#a-hands-on-example" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>At BraSCCH (Brazilian Smart Care for Child Health), we most of the time deal with training Machine Learning models that can help us on the task of identifying neonatal death.</p><p>For this task, we use a dataset that contains several features related to a given newborn, such as: mother’s age, the newborn weight, vital signs (such as breathing, muscular tonus, appearance, etc). What if we wanted to know, even when using a complex model, such as a Extreme Gradient Boosting, or a Neural Network, which are the features that play a major role in the prediction result?</p><p>For this task, we will be used the concept of SHAPley values, implemented on the <a href="https://github.com/slundberg/shap">SHAP library</a>, which is primarily developed in Python and can be installed directly from pip (pip install shap and that’s all).</p><h2 id="displaying-a-summary-plot"><span class="me-2">Displaying a summary plot</span><a href="#displaying-a-summary-plot" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>In order to have a clear vision of the features and their impact on our model, we can use a summary plot, which sorts the features by their SHAP values, representing their influence on the model output.</p><p><a href="/assets/img/posts/02-shap/summary_plot.png" class="popup img-link "><img data-src="/assets/img/posts/02-shap/summary_plot.png" class="lazyload" data-proofer-ignore></a></p><p>By displaying our summary plot, we can see that the features that play a major role in the model decision are:</p><ul><li>Newborn weight<li>APGAR 5 (vital signs of the newborn at the 5th minute of life), if there was any identified anomaly<li>APGAR 1 (similar to APGAR 5, but at the 1st minute)<li>Weeks of pregnancy and number of prenatal consultations.</ul><p>Just an example to explain one of the listed features, we can observe that for the feature weight, as it goes higher in value, smaller is the SHAP value for it, thus indicating that newborns with a large weight have a smaller impact on the classifier output, while that newborns with a lower weight do contribute largely to an increased SHAP value and consequently to the model final prediction.</p><p>We can also see this same summary plot in a more simplistic manner, by specifying the plot_type parameter as <code class="language-plaintext highlighter-rouge">bar</code>.</p><p><a href="/assets/img/posts/02-shap/bar_plot.png" class="popup img-link "><img data-src="/assets/img/posts/02-shap/bar_plot.png" class="lazyload" data-proofer-ignore></a></p><p>This plot conveys pretty much a simpler explanation about our model, just displaying the features and their averaged impacts.</p><h2 id="the-dependence-plot"><span class="me-2">The dependence plot</span><a href="#the-dependence-plot" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Another highly used plot when trying to understand the correlation between the features and the model outcomes is the dependence plot. This plot allows us to see how a given feature impacts the SHAP value along with the distribution of another feature (commonly referred as interaction feature).</p><p><a href="/assets/img/posts/02-shap/dependence_plot.png" class="popup img-link "><img data-src="/assets/img/posts/02-shap/dependence_plot.png" class="lazyload" data-proofer-ignore></a></p><p>In this plot, we display the mother’s education in years (x axis), correlated with the respective SHAP values (y axis) and the interaction with the feature “weeks of pregnancy” (color bar at right).</p><p>We can clearly notice that, the higher the education of the mother, less likely it is for the newborn to die. This may likely happen due to socio-economical factors, since more educated mothers are more likely to have a higher income and thus better living conditions.</p><p>Related to this, we can see how the weeks of pregnancy is distributed over the given education category groups. We can notice that for mothers with a high education, the major factor of dying is most likely to be related to a low number of weeks of pregnancy, while that this pattern is not displayed for the other groups.</p><h2 id="conclusions"><span class="me-2">Conclusions</span><a href="#conclusions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>In this post, we briefly went through one of the tools used to explain Machine Learning models, but there are several others, such as <a href="https://github.com/marcotcr/lime">Lime</a> and <a href="https://github.com/microsoft/interpret">Interpret</a>. I recommend everyone to play a little bit with those, to have a better grasp of how these tools can better help us to understand not only about our model outcomes, but also about our given problem.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/data-science/">data science</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/data-science/" class="post-tag no-text-decoration" >data science</a> <a href="/tags/feature-engineering/" class="post-tag no-text-decoration" >feature engineering</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >machine learning</a> <a href="/tags/python/" class="post-tag no-text-decoration" >python</a> <a href="/tags/pandas/" class="post-tag no-text-decoration" >pandas</a> <a href="/tags/scikit-learn/" class="post-tag no-text-decoration" >scikit-learn</a> <a href="/tags/tensorflow/" class="post-tag no-text-decoration" >tensorflow</a> <a href="/tags/keras/" class="post-tag no-text-decoration" >keras</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted me-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Elucidating%20Machine%20Learning%20models%20with%20SHAP%20values%20-%20Rodrigo%20Bressan&url=https%3A%2F%2Fbressan.xyz%2Fposts%2Fshap%2F" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter" > <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Elucidating%20Machine%20Learning%20models%20with%20SHAP%20values%20-%20Rodrigo%20Bressan&u=https%3A%2F%2Fbressan.xyz%2Fposts%2Fshap%2F" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook" > <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fbressan.xyz%2Fposts%2Fshap%2F&text=Elucidating%20Machine%20Learning%20models%20with%20SHAP%20values%20-%20Rodrigo%20Bressan" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram" > <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fbressan.xyz%2Fposts%2Fshap%2F" data-bs-toggle="tooltip" data-bs-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin" > <i class="fa-fw fab fa-linkedin"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/lynis-audit/">Navigating HIPAA compliance on the Cloud with Lynis</a><li class="text-truncate lh-lg"> <a href="/posts/kopf-k8s-operator/">Simplifying Kubernetes Operators with Kopf: A Quick Guide</a><li class="text-truncate lh-lg"> <a href="/posts/k8s-commands/">Essential kubectl Commands</a><li class="text-truncate lh-lg"> <a href="/posts/nlp-basics/">NLP with Python: A Beginner's Guide</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/data-science/">data science</a> <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a> <a class="post-tag btn btn-outline-primary" href="/tags/cloud/">cloud</a> <a class="post-tag btn btn-outline-primary" href="/tags/feature-engineering/">feature engineering</a> <a class="post-tag btn btn-outline-primary" href="/tags/keras/">keras</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning/">machine learning</a> <a class="post-tag btn btn-outline-primary" href="/tags/pandas/">pandas</a> <a class="post-tag btn btn-outline-primary" href="/tags/scikit-learn/">scikit-learn</a> <a class="post-tag btn btn-outline-primary" href="/tags/tensorflow/">tensorflow</a> <a class="post-tag btn btn-outline-primary" href="/tags/devops/">devops</a></div></section></div><section id="toc-wrapper" class="ps-0 pe-4 mb-5"><h2 class="panel-heading ps-3 pt-2 mb-2">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/entity-embeddings/" class="post-preview card h-100"><div class="card-body"> <time class="small" data-ts="1640008800" data-df="ll" > Dec 20, 2021 </time><h4 class="pt-0 my-2">Enhancing Categorical Features with Entity Embeddings</h4><div class="text-muted small"><p> Let’s talk about selling beers. Let’s suppose you are the owner of a pub, and you would like to predict how many beers your establishment is going to sell on a given day based on two variables: ...</p></div></div></a></article><article class="col"> <a href="/posts/keras-101-boston/" class="post-preview card h-100"><div class="card-body"> <time class="small" data-ts="1644933600" data-df="ll" > Feb 15, 2022 </time><h4 class="pt-0 my-2">Keras 101: A simple Neural Network for House Pricing regression</h4><div class="text-muted small"><p> In this post, we will be covering some basics of data exploration and building a model with Keras in order to help us on predicting the selling price of a given house in the Boston (MA) area. As an...</p></div></div></a></article><article class="col"> <a href="/posts/keras-multi-output/" class="post-preview card h-100"><div class="card-body"> <time class="small" data-ts="1647352800" data-df="ll" > Mar 15, 2022 </time><h4 class="pt-0 my-2">Building a multi-output Convolutional Neural Network with Keras</h4><div class="text-muted small"><p> In this post, we will be exploring the Keras functional API in order to build a multi-output Deep Learning model. We will show how to train a single model that is capable of predicting three distin...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/entity-embeddings/" class="btn btn-outline-primary" aria-label="Older" ><p>Enhancing Categorical Features with Entity Embeddings</p></a> <a href="/posts/keras-101-boston/" class="btn btn-outline-primary" aria-label="Newer" ><p>Keras 101: A simple Neural Network for House Pricing regression</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p> © <time>2025</time> <a href="https://github.com/rodrigobressan">Rodrigo Bressan</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>See something wrong? Want to contribute? <a href="https://github.com/rodrigobressan/rodrigobressan.github.io/tree/master/_posts">Edit this page 📝</href></p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/data-science/">data science</a> <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a> <a class="post-tag btn btn-outline-primary" href="/tags/cloud/">cloud</a> <a class="post-tag btn btn-outline-primary" href="/tags/feature-engineering/">feature engineering</a> <a class="post-tag btn btn-outline-primary" href="/tags/keras/">keras</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning/">machine learning</a> <a class="post-tag btn btn-outline-primary" href="/tags/pandas/">pandas</a> <a class="post-tag btn btn-outline-primary" href="/tags/scikit-learn/">scikit-learn</a> <a class="post-tag btn btn-outline-primary" href="/tags/tensorflow/">tensorflow</a> <a class="post-tag btn btn-outline-primary" href="/tags/devops/">devops</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask"></div><script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.1/dist/jquery.min.js,npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.9/dayjs.min.js,npm/dayjs@1.11.9/locale/en.min.js,npm/dayjs@1.11.9/plugin/relativeTime.min.js,npm/dayjs@1.11.9/plugin/localizedFormat.min.js,npm/tocbot@4.21.1/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/unregister.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-5B9QC89SHN"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-5B9QC89SHN'); }); </script> <script> /* Note: dependent library will be loaded in `js-selector.html` */ SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5"></p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
