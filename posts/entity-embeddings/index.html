<!doctype html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Enhancing Categorical Features with Entity Embeddings" /><meta property="og:locale" content="en" /><meta name="description" content="Let’s talk about selling beers. Let’s suppose you are the owner of a pub, and you would like to predict how many beers your establishment is going to sell on a given day based on two variables: the day of the week the current weather We can probably hypothesize that weekends and warm days are going to sell more beers when compared to the beginning of the week and cold days, right? Let’s see if this hypothesis holds to be true In face of this problem, we usually would start by encoding our categorical data (in this example, the day of the week and the weather) into dummy variables, in order to provide an input to our classifier without any kind of relationship between the existing categorical values. Our data would look like something below, for the day of week feature (you can imagine something similar for the weather feature): But does this really makes sense, to treat each categorical value as being completely different from one another, such as when using One-Hot-Encoding? Or even better: Could we make usage of some sort of technique to “learn” the similarities between each possible value and their outcome? Entity Embeddings to the rescue With this given scenario in mind, we can then proceed to the adoption of a technique popularly known in the NLP (Natural Language Processing) field as Entity Embeddings, which allows us to map a given feature set into a new one with a smaller number of dimensions. In our case, it will also allow us to extract meaningful information from our categorical data. The usage of Entity Embeddings is based on the process of training of a Neural Network with the categorical data, with the ultimate purpose to retrieve the weights of the Embedding layers, allowing us to have a more significant input when compared to a single One-Hot-Encoding approach. By adopting Entity Embeddings we also are able to mitigate two major problems: No need to have a domain expert, once we’re capable to train a Neural Network that can efficiently learn patterns and relationships between the values of a same categorical feature, thus removing the step of feature engineering (such as manually giving weights to each day of the week or kind of weather); Shrinkage on computing resources, once we’re not longer just directly encoding our possible categorical values with One-Hot-Encoding, which can represent a huge resource usage: Let’s just suppose you have a categorical feature with 10 thousand possible unique values. This would translate into a feature vector with the same amount of empty positions just to represent a given value. The definition of Entity Embedding In short words, the Embedding layer is pretty much a Neural Network layer that groups, in a N-dimensional space, categorical values with similar output value. This spatial representation allows us to obtain intrinsic properties of each categorical value, which can be later on used as a replacement to our old dummy encoded variables. If we think about it in a more simple manner, it would mean that days of the week that have a similar output (in our case, number of sold beers), would be close to each other. If you don’t get it, a picture can help: Here we can see that we have four major groups: Group 1, with Monday and Tuesday, possibly related to a low amount of sold beers, due to being the start of the week Group 2, with Wednesday and Thursday, with some distance from group 1 Group 3, with Friday and Saturday, relatively close to group 2, indicating that they show more similarity than when compared with group 1 Group 4, with Sunday, without many similarities to the other groups This simple example can show us that the embedding layers can learn information from the real world, such as the most common days for going out and drinking. Pretty cool, isn’t it? Putting it together with Keras First of all, we need to know that for using an embedding layer, we must specify the number of dimensions we would like to be used for that given embedding. This, as you can notice, is a hyperparameter, and should be tested and experimented case by case. But as a rule of thumb, you can adopt the number of dimensions as equal to the square root of the number of unique values for the category. So in our case, our representation for the day of the week would have instead of seven different positions, only three (we round up in our case, since the square root of 7 is 2.64). Below we give an example for both mentioned features, as well as add some hidden layers, in order to have more parameters in our model to capture minor data nuances. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # Embedding layer for the &#39;Day of Week&#39; feature n_unique_day = df[&#39;Day&#39;].nunique() n_dim_day = int(sqrt(n_unique_day)) input_week = Input(shape=(1, )) output_week = Embedding(input_dim=n_unique_day, output_dim=n_dim_day, name=&quot;day&quot;)(input_week) output_week = Reshape(target_shape=(n_dim_day, ))(output_week) # Embedding layer for the &#39;Weather&#39; feature n_unique_weather = df[&#39;Weather&#39;].nunique() n_dim_weather = int(sqrt(n_unique_weather)) input_weather = Input(shape=(1, )) output_weather = Embedding(input_dim=n_unique_weather, output_dim=n_dim_weather, name=&quot;weather&quot;)(input_weather) output_weather = Reshape(target_shape=(n_dim_weather,))(output_weather) input_layers = [input_week, input_weather] output_layers = [output_week, output_weather] model = Concatenate()(output_layers) # Add a few hidden layers model = Dense(200, kernel_initializer=&quot;uniform&quot;)(model) model = Activation(&#39;relu&#39;)(model) model = Dense(100, kernel_initializer=&quot;uniform&quot;)(model) model = Activation(&#39;relu&#39;)(model) # And finally our output layer model = Dense(1)(model) model = Activation(&#39;sigmoid&#39;)(model) # Put it all together and compile the model model = KerasModel(inputs=input_layers, outputs=model) model.summary() opt = SGD(lr=0.05) model.compile(loss=&#39;mse&#39;, optimizer=opt, metrics=[&#39;mse&#39;]) Graphically our Neural Network would have the following representation: Results That’s it. We can see that our architecture is composed from a Input layer for each of the categorical values, followed by our Embedding layers, then a Reshape layer and then all put together. Lastly, we add some hidden layers to capture minor nuances of our data. Training our network for 200 epochs with a learning rate of 0.05, we can see some pretty good results for loss and mean squared error: Conclusions In this example it may sound silly, but we can again think about our scenario of 10 thousand unique values. The difference between a feature vector with 10 thousand positions (by using One-Hot-Encoding) and another with only 100 (measured by the rule of thumb, when using entity embeddings) is enormous. This is the difference for only a single record for a single feature. You can imagine how complex this becomes with a real world dataset. If you reached until this point without any doubts, congratulations! But if you do have any kind of questions, suggestions or complains, feel free to reach me Source Code If you want to check the full source code for this example, you can find it in my GitHub" /><meta property="og:description" content="Let’s talk about selling beers. Let’s suppose you are the owner of a pub, and you would like to predict how many beers your establishment is going to sell on a given day based on two variables: the day of the week the current weather We can probably hypothesize that weekends and warm days are going to sell more beers when compared to the beginning of the week and cold days, right? Let’s see if this hypothesis holds to be true In face of this problem, we usually would start by encoding our categorical data (in this example, the day of the week and the weather) into dummy variables, in order to provide an input to our classifier without any kind of relationship between the existing categorical values. Our data would look like something below, for the day of week feature (you can imagine something similar for the weather feature): But does this really makes sense, to treat each categorical value as being completely different from one another, such as when using One-Hot-Encoding? Or even better: Could we make usage of some sort of technique to “learn” the similarities between each possible value and their outcome? Entity Embeddings to the rescue With this given scenario in mind, we can then proceed to the adoption of a technique popularly known in the NLP (Natural Language Processing) field as Entity Embeddings, which allows us to map a given feature set into a new one with a smaller number of dimensions. In our case, it will also allow us to extract meaningful information from our categorical data. The usage of Entity Embeddings is based on the process of training of a Neural Network with the categorical data, with the ultimate purpose to retrieve the weights of the Embedding layers, allowing us to have a more significant input when compared to a single One-Hot-Encoding approach. By adopting Entity Embeddings we also are able to mitigate two major problems: No need to have a domain expert, once we’re capable to train a Neural Network that can efficiently learn patterns and relationships between the values of a same categorical feature, thus removing the step of feature engineering (such as manually giving weights to each day of the week or kind of weather); Shrinkage on computing resources, once we’re not longer just directly encoding our possible categorical values with One-Hot-Encoding, which can represent a huge resource usage: Let’s just suppose you have a categorical feature with 10 thousand possible unique values. This would translate into a feature vector with the same amount of empty positions just to represent a given value. The definition of Entity Embedding In short words, the Embedding layer is pretty much a Neural Network layer that groups, in a N-dimensional space, categorical values with similar output value. This spatial representation allows us to obtain intrinsic properties of each categorical value, which can be later on used as a replacement to our old dummy encoded variables. If we think about it in a more simple manner, it would mean that days of the week that have a similar output (in our case, number of sold beers), would be close to each other. If you don’t get it, a picture can help: Here we can see that we have four major groups: Group 1, with Monday and Tuesday, possibly related to a low amount of sold beers, due to being the start of the week Group 2, with Wednesday and Thursday, with some distance from group 1 Group 3, with Friday and Saturday, relatively close to group 2, indicating that they show more similarity than when compared with group 1 Group 4, with Sunday, without many similarities to the other groups This simple example can show us that the embedding layers can learn information from the real world, such as the most common days for going out and drinking. Pretty cool, isn’t it? Putting it together with Keras First of all, we need to know that for using an embedding layer, we must specify the number of dimensions we would like to be used for that given embedding. This, as you can notice, is a hyperparameter, and should be tested and experimented case by case. But as a rule of thumb, you can adopt the number of dimensions as equal to the square root of the number of unique values for the category. So in our case, our representation for the day of the week would have instead of seven different positions, only three (we round up in our case, since the square root of 7 is 2.64). Below we give an example for both mentioned features, as well as add some hidden layers, in order to have more parameters in our model to capture minor data nuances. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # Embedding layer for the &#39;Day of Week&#39; feature n_unique_day = df[&#39;Day&#39;].nunique() n_dim_day = int(sqrt(n_unique_day)) input_week = Input(shape=(1, )) output_week = Embedding(input_dim=n_unique_day, output_dim=n_dim_day, name=&quot;day&quot;)(input_week) output_week = Reshape(target_shape=(n_dim_day, ))(output_week) # Embedding layer for the &#39;Weather&#39; feature n_unique_weather = df[&#39;Weather&#39;].nunique() n_dim_weather = int(sqrt(n_unique_weather)) input_weather = Input(shape=(1, )) output_weather = Embedding(input_dim=n_unique_weather, output_dim=n_dim_weather, name=&quot;weather&quot;)(input_weather) output_weather = Reshape(target_shape=(n_dim_weather,))(output_weather) input_layers = [input_week, input_weather] output_layers = [output_week, output_weather] model = Concatenate()(output_layers) # Add a few hidden layers model = Dense(200, kernel_initializer=&quot;uniform&quot;)(model) model = Activation(&#39;relu&#39;)(model) model = Dense(100, kernel_initializer=&quot;uniform&quot;)(model) model = Activation(&#39;relu&#39;)(model) # And finally our output layer model = Dense(1)(model) model = Activation(&#39;sigmoid&#39;)(model) # Put it all together and compile the model model = KerasModel(inputs=input_layers, outputs=model) model.summary() opt = SGD(lr=0.05) model.compile(loss=&#39;mse&#39;, optimizer=opt, metrics=[&#39;mse&#39;]) Graphically our Neural Network would have the following representation: Results That’s it. We can see that our architecture is composed from a Input layer for each of the categorical values, followed by our Embedding layers, then a Reshape layer and then all put together. Lastly, we add some hidden layers to capture minor nuances of our data. Training our network for 200 epochs with a learning rate of 0.05, we can see some pretty good results for loss and mean squared error: Conclusions In this example it may sound silly, but we can again think about our scenario of 10 thousand unique values. The difference between a feature vector with 10 thousand positions (by using One-Hot-Encoding) and another with only 100 (measured by the rule of thumb, when using entity embeddings) is enormous. This is the difference for only a single record for a single feature. You can imagine how complex this becomes with a real world dataset. If you reached until this point without any doubts, congratulations! But if you do have any kind of questions, suggestions or complains, feel free to reach me Source Code If you want to check the full source code for this example, you can find it in my GitHub" /><link rel="canonical" href="https://bressan.xyz/posts/entity-embeddings/" /><meta property="og:url" content="https://bressan.xyz/posts/entity-embeddings/" /><meta property="og:site_name" content="Rodrigo Bressan" /><meta property="og:image" content="https://bressan.xyz/assets/img/posts/01-embeddings/banner.jpeg" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-12-20T15:00:00+01:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://bressan.xyz/assets/img/posts/01-embeddings/banner.jpeg" /><meta property="twitter:title" content="Enhancing Categorical Features with Entity Embeddings" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-12-20T15:00:00+01:00","datePublished":"2021-12-20T15:00:00+01:00","description":"Let’s talk about selling beers. Let’s suppose you are the owner of a pub, and you would like to predict how many beers your establishment is going to sell on a given day based on two variables: the day of the week the current weather We can probably hypothesize that weekends and warm days are going to sell more beers when compared to the beginning of the week and cold days, right? Let’s see if this hypothesis holds to be true In face of this problem, we usually would start by encoding our categorical data (in this example, the day of the week and the weather) into dummy variables, in order to provide an input to our classifier without any kind of relationship between the existing categorical values. Our data would look like something below, for the day of week feature (you can imagine something similar for the weather feature): But does this really makes sense, to treat each categorical value as being completely different from one another, such as when using One-Hot-Encoding? Or even better: Could we make usage of some sort of technique to “learn” the similarities between each possible value and their outcome? Entity Embeddings to the rescue With this given scenario in mind, we can then proceed to the adoption of a technique popularly known in the NLP (Natural Language Processing) field as Entity Embeddings, which allows us to map a given feature set into a new one with a smaller number of dimensions. In our case, it will also allow us to extract meaningful information from our categorical data. The usage of Entity Embeddings is based on the process of training of a Neural Network with the categorical data, with the ultimate purpose to retrieve the weights of the Embedding layers, allowing us to have a more significant input when compared to a single One-Hot-Encoding approach. By adopting Entity Embeddings we also are able to mitigate two major problems: No need to have a domain expert, once we’re capable to train a Neural Network that can efficiently learn patterns and relationships between the values of a same categorical feature, thus removing the step of feature engineering (such as manually giving weights to each day of the week or kind of weather); Shrinkage on computing resources, once we’re not longer just directly encoding our possible categorical values with One-Hot-Encoding, which can represent a huge resource usage: Let’s just suppose you have a categorical feature with 10 thousand possible unique values. This would translate into a feature vector with the same amount of empty positions just to represent a given value. The definition of Entity Embedding In short words, the Embedding layer is pretty much a Neural Network layer that groups, in a N-dimensional space, categorical values with similar output value. This spatial representation allows us to obtain intrinsic properties of each categorical value, which can be later on used as a replacement to our old dummy encoded variables. If we think about it in a more simple manner, it would mean that days of the week that have a similar output (in our case, number of sold beers), would be close to each other. If you don’t get it, a picture can help: Here we can see that we have four major groups: Group 1, with Monday and Tuesday, possibly related to a low amount of sold beers, due to being the start of the week Group 2, with Wednesday and Thursday, with some distance from group 1 Group 3, with Friday and Saturday, relatively close to group 2, indicating that they show more similarity than when compared with group 1 Group 4, with Sunday, without many similarities to the other groups This simple example can show us that the embedding layers can learn information from the real world, such as the most common days for going out and drinking. Pretty cool, isn’t it? Putting it together with Keras First of all, we need to know that for using an embedding layer, we must specify the number of dimensions we would like to be used for that given embedding. This, as you can notice, is a hyperparameter, and should be tested and experimented case by case. But as a rule of thumb, you can adopt the number of dimensions as equal to the square root of the number of unique values for the category. So in our case, our representation for the day of the week would have instead of seven different positions, only three (we round up in our case, since the square root of 7 is 2.64). Below we give an example for both mentioned features, as well as add some hidden layers, in order to have more parameters in our model to capture minor data nuances. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # Embedding layer for the &#39;Day of Week&#39; feature n_unique_day = df[&#39;Day&#39;].nunique() n_dim_day = int(sqrt(n_unique_day)) input_week = Input(shape=(1, )) output_week = Embedding(input_dim=n_unique_day, output_dim=n_dim_day, name=&quot;day&quot;)(input_week) output_week = Reshape(target_shape=(n_dim_day, ))(output_week) # Embedding layer for the &#39;Weather&#39; feature n_unique_weather = df[&#39;Weather&#39;].nunique() n_dim_weather = int(sqrt(n_unique_weather)) input_weather = Input(shape=(1, )) output_weather = Embedding(input_dim=n_unique_weather, output_dim=n_dim_weather, name=&quot;weather&quot;)(input_weather) output_weather = Reshape(target_shape=(n_dim_weather,))(output_weather) input_layers = [input_week, input_weather] output_layers = [output_week, output_weather] model = Concatenate()(output_layers) # Add a few hidden layers model = Dense(200, kernel_initializer=&quot;uniform&quot;)(model) model = Activation(&#39;relu&#39;)(model) model = Dense(100, kernel_initializer=&quot;uniform&quot;)(model) model = Activation(&#39;relu&#39;)(model) # And finally our output layer model = Dense(1)(model) model = Activation(&#39;sigmoid&#39;)(model) # Put it all together and compile the model model = KerasModel(inputs=input_layers, outputs=model) model.summary() opt = SGD(lr=0.05) model.compile(loss=&#39;mse&#39;, optimizer=opt, metrics=[&#39;mse&#39;]) Graphically our Neural Network would have the following representation: Results That’s it. We can see that our architecture is composed from a Input layer for each of the categorical values, followed by our Embedding layers, then a Reshape layer and then all put together. Lastly, we add some hidden layers to capture minor nuances of our data. Training our network for 200 epochs with a learning rate of 0.05, we can see some pretty good results for loss and mean squared error: Conclusions In this example it may sound silly, but we can again think about our scenario of 10 thousand unique values. The difference between a feature vector with 10 thousand positions (by using One-Hot-Encoding) and another with only 100 (measured by the rule of thumb, when using entity embeddings) is enormous. This is the difference for only a single record for a single feature. You can imagine how complex this becomes with a real world dataset. If you reached until this point without any doubts, congratulations! But if you do have any kind of questions, suggestions or complains, feel free to reach me Source Code If you want to check the full source code for this example, you can find it in my GitHub","headline":"Enhancing Categorical Features with Entity Embeddings","image":"https://bressan.xyz/assets/img/posts/01-embeddings/banner.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"https://bressan.xyz/posts/entity-embeddings/"},"url":"https://bressan.xyz/posts/entity-embeddings/"}</script><title>Enhancing Categorical Features with Entity Embeddings | Rodrigo Bressan</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Rodrigo Bressan"><meta name="application-name" content="Rodrigo Bressan"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.1/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"> <img src="/assets/img/me.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"> </a><h1 class="site-title"> <a href="/">Rodrigo Bressan</a></h1><p class="site-subtitle fst-italic mb-0">Software, Healthcare and Improving People's lives</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a><li class="nav-item"> <a href="/contact/" class="nav-link"> <i class="fa-fw fas fa-address-card"></i> <span>CONTACT</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fas fa-code"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/resume/" class="nav-link"> <i class="fa-fw fas fa-file"></i> <span>RESUME</span> </a><li class="nav-item"> <a href="/all-posts/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ALL POSTS</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <a href="https://github.com/rodrigobressan" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://www.linkedin.com/in/rbressan/" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container px-xxl-5"><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Enhancing Categorical Features with Entity Embeddings</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4" ><article class="px-1"><header><h1 data-toc-skip>Enhancing Categorical Features with Entity Embeddings</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1640008800" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Dec 20, 2021 </time> </span><div class="mt-3 mb-3"> <a href="/assets/img/posts/01-embeddings/banner.jpeg" class="popup img-link preview-img shimmer"><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1200 630'%3E%3C/svg%3E" data-src="/assets/img/posts/01-embeddings/banner.jpeg" alt="Preview Image" width="1200" height="630" class="lazyload" data-proofer-ignore></a></div><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/rodrigobressan">Rodrigo Bressan</a> </em> </span> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="1157 words" > <em>6 min</em> read</span></div></div></header><div class="content"><h2 id="lets-talk-about-selling-beers"><span class="me-2">Let’s talk about selling beers.</span><a href="#lets-talk-about-selling-beers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Let’s suppose you are the owner of a pub, and you would like to predict how many beers your establishment is going to sell on a given day based on two variables:</p><ul><li>the day of the week<li><p>the current weather</p><li>We can probably hypothesize that weekends and warm days are going to sell more beers when compared to the beginning of the week and cold days, right? Let’s see if this hypothesis holds to be true</ul><p>In face of this problem, we usually would start by encoding our categorical data (<i>in this example, the day of the week and the weather</i>) into dummy variables, in order to provide an input to our classifier without any kind of relationship between the existing categorical values.</p><p>Our data would look like something below, for the day of week feature (you can imagine something similar for the weather feature):</p><p><a href="/assets/img/posts/01-embeddings/emb_dummy.png" class="popup img-link "><img style="width: 100%; object-fit: contain" data-src="/assets/img/posts/01-embeddings/emb_dummy.png" class="lazyload" data-proofer-ignore></a></p><p><b>But does this really makes sense, to treat each categorical value as being completely different from one another</b>, such as when using One-Hot-Encoding? Or even better:</p><p><b>Could we make usage of some sort of technique to “learn” the similarities between each possible value and their outcome?</b></p><h2 id="entity-embeddings-to-the-rescue"><span class="me-2">Entity Embeddings to the rescue</span><a href="#entity-embeddings-to-the-rescue" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>With this given scenario in mind, we can then proceed to the adoption of a technique popularly known in the NLP (Natural Language Processing) field as Entity Embeddings, which allows us to map a given feature set into a new one with a smaller number of dimensions. In our case, it will also allow us to extract meaningful information from our categorical data.</p><p>The usage of Entity Embeddings is based on the process of training of a Neural Network with the categorical data, with the ultimate purpose to retrieve the weights of the Embedding layers, allowing us to have a more significant input when compared to a single One-Hot-Encoding approach. By adopting Entity Embeddings we also are able to mitigate two major problems:</p><ul><li><b>No need to have a domain expert</b>, once we’re capable to train a Neural Network that can efficiently learn patterns and relationships between the values of a same categorical feature, thus removing the step of feature engineering (such as manually giving weights to each day of the week or kind of weather);<li><b>Shrinkage on computing resources</b>, once we’re not longer just directly encoding our possible categorical values with One-Hot-Encoding, which can represent a huge resource usage: Let’s just suppose you have a categorical feature with 10 thousand possible unique values. This would translate into a feature vector with the same amount of empty positions just to represent a given value.</ul><p><br /><br /></p><h2 id="the-definition-of-entity-embedding"><span class="me-2">The definition of Entity Embedding</span><a href="#the-definition-of-entity-embedding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>In short words, the Embedding layer is pretty much a Neural Network layer that groups, in a N-dimensional space, categorical values with similar output value. This spatial representation allows us to obtain intrinsic properties of each categorical value, which can be later on used as a replacement to our old dummy encoded variables. If we think about it in a more simple manner, it would mean that days of the week that have a similar output (in our case, number of sold beers), would be close to each other. If you don’t get it, a picture can help:</p><p><a href="/assets/img/posts/01-embeddings/emb_plot.png" class="popup img-link "><img data-src="/assets/img/posts/01-embeddings/emb_plot.png" class="lazyload" data-proofer-ignore></a></p><p>Here we can see that we have four major groups:</p><ul><li>Group 1, with Monday and Tuesday, possibly related to a low amount of sold beers, due to being the start of the week<li>Group 2, with Wednesday and Thursday, with some distance from group 1<li>Group 3, with Friday and Saturday, relatively close to group 2, indicating that they show more similarity than when compared with group 1<li>Group 4, with Sunday, without many similarities to the other groups</ul><p><b>This simple example can show us that the embedding layers can learn information from the real world, such as the most common days for going out and drinking. Pretty cool, isn’t it?</b></p><h2 id="putting-it-together-with-keras"><span class="me-2">Putting it together with Keras</span><a href="#putting-it-together-with-keras" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>First of all, we need to know that for using an embedding layer, we must specify the number of dimensions we would like to be used for that given embedding. This, as you can notice, is a hyperparameter, and should be tested and experimented case by case. But as a rule of thumb, you can adopt the number of dimensions as equal to the square root of the number of unique values for the category. So in our case, our representation for the day of the week would have instead of seven different positions, only three (we round up in our case, since the square root of 7 is 2.64). Below we give an example for both mentioned features, as well as add some hidden layers, in order to have more parameters in our model to capture minor data nuances.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
</pre><td class="rouge-code"><pre><span class="c1"># Embedding layer for the 'Day of Week' feature
</span><span class="n">n_unique_day</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">Day</span><span class="sh">'</span><span class="p">].</span><span class="nf">nunique</span><span class="p">()</span>
<span class="n">n_dim_day</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">n_unique_day</span><span class="p">))</span>

<span class="n">input_week</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">))</span>
<span class="n">output_week</span> <span class="o">=</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">n_unique_day</span><span class="p">,</span> 
                        <span class="n">output_dim</span><span class="o">=</span><span class="n">n_dim_day</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">day</span><span class="sh">"</span><span class="p">)(</span><span class="n">input_week</span><span class="p">)</span>
<span class="n">output_week</span> <span class="o">=</span> <span class="nc">Reshape</span><span class="p">(</span><span class="n">target_shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_dim_day</span><span class="p">,</span> <span class="p">))(</span><span class="n">output_week</span><span class="p">)</span>

<span class="c1"># Embedding layer for the 'Weather' feature
</span><span class="n">n_unique_weather</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">Weather</span><span class="sh">'</span><span class="p">].</span><span class="nf">nunique</span><span class="p">()</span>
<span class="n">n_dim_weather</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">n_unique_weather</span><span class="p">))</span>

<span class="n">input_weather</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">))</span>
<span class="n">output_weather</span> <span class="o">=</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">n_unique_weather</span><span class="p">,</span> 
                           <span class="n">output_dim</span><span class="o">=</span><span class="n">n_dim_weather</span><span class="p">,</span> 
                           <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weather</span><span class="sh">"</span><span class="p">)(</span><span class="n">input_weather</span><span class="p">)</span>

<span class="n">output_weather</span> <span class="o">=</span> <span class="nc">Reshape</span><span class="p">(</span><span class="n">target_shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_dim_weather</span><span class="p">,))(</span><span class="n">output_weather</span><span class="p">)</span>

<span class="n">input_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_week</span><span class="p">,</span> <span class="n">input_weather</span><span class="p">]</span>
<span class="n">output_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">output_week</span><span class="p">,</span> <span class="n">output_weather</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Concatenate</span><span class="p">()(</span><span class="n">output_layers</span><span class="p">)</span>

<span class="c1"># Add a few hidden layers
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="sh">"</span><span class="s">uniform</span><span class="sh">"</span><span class="p">)(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">model</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="sh">"</span><span class="s">uniform</span><span class="sh">"</span><span class="p">)(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># And finally our output layer
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Put it all together and compile the model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">KerasModel</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layers</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>

<span class="n">opt</span> <span class="o">=</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">mse</span><span class="sh">'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">mse</span><span class="sh">'</span><span class="p">])</span>
</pre></table></code></div></div><p>Graphically our Neural Network would have the following representation:</p><p><a href="/assets/img/posts/01-embeddings/emb_architecture.png" class="popup img-link "><img style="height: 100%; width: 100%; object-fit: contain" data-src="/assets/img/posts/01-embeddings/emb_architecture.png" class="lazyload" data-proofer-ignore></a></p><h2 id="results"><span class="me-2">Results</span><a href="#results" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>That’s it. We can see that our architecture is composed from a Input layer for each of the categorical values, followed by our Embedding layers, then a Reshape layer and then all put together. Lastly, we add some hidden layers to capture minor nuances of our data. Training our network for <b>200 epochs with a learning rate of 0.05</b>, we can see some pretty good results for loss and mean squared error:</p><p><a href="/assets/img/posts/01-embeddings/emb_loss.png" class="popup img-link "><img data-src="/assets/img/posts/01-embeddings/emb_loss.png" class="lazyload" data-proofer-ignore></a> <a href="/assets/img/posts/01-embeddings/emb_mse.png" class="popup img-link "><img data-src="/assets/img/posts/01-embeddings/emb_mse.png" class="lazyload" data-proofer-ignore></a></p><h2 id="conclusions"><span class="me-2">Conclusions</span><a href="#conclusions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>In this example it may sound silly, but we can again think about our scenario of 10 thousand unique values. The difference between a feature vector with 10 thousand positions (by using One-Hot-Encoding) and another with only 100 (measured by the rule of thumb, when using entity embeddings) is enormous. This is the difference for only a single record for a single feature. You can imagine how complex this becomes with a real world dataset.</p><p>If you reached until this point without any doubts, congratulations! But if you do have any kind of questions, suggestions or complains, feel free to reach me</p><h2 id="source-code"><span class="me-2">Source Code</span><a href="#source-code" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>If you want to check the full source code for this example, you can find it in my <a href="https://github.com/rodrigobressan/entity_embeddings_categorical">GitHub</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/data-science/">data science</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/data-science/" class="post-tag no-text-decoration" >data science</a> <a href="/tags/feature-engineering/" class="post-tag no-text-decoration" >feature engineering</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >machine learning</a> <a href="/tags/python/" class="post-tag no-text-decoration" >python</a> <a href="/tags/pandas/" class="post-tag no-text-decoration" >pandas</a> <a href="/tags/scikit-learn/" class="post-tag no-text-decoration" >scikit-learn</a> <a href="/tags/tensorflow/" class="post-tag no-text-decoration" >tensorflow</a> <a href="/tags/keras/" class="post-tag no-text-decoration" >keras</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted me-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Enhancing%20Categorical%20Features%20with%20Entity%20Embeddings%20-%20Rodrigo%20Bressan&url=https%3A%2F%2Fbressan.xyz%2Fposts%2Fentity-embeddings%2F" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter" > <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Enhancing%20Categorical%20Features%20with%20Entity%20Embeddings%20-%20Rodrigo%20Bressan&u=https%3A%2F%2Fbressan.xyz%2Fposts%2Fentity-embeddings%2F" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook" > <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fbressan.xyz%2Fposts%2Fentity-embeddings%2F&text=Enhancing%20Categorical%20Features%20with%20Entity%20Embeddings%20-%20Rodrigo%20Bressan" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram" > <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fbressan.xyz%2Fposts%2Fentity-embeddings%2F" data-bs-toggle="tooltip" data-bs-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin" > <i class="fa-fw fab fa-linkedin"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/lynis-audit/">Navigating HIPAA compliance on the Cloud with Lynis</a><li class="text-truncate lh-lg"> <a href="/posts/kopf-k8s-operator/">Simplifying Kubernetes Operators with Kopf: A Quick Guide</a><li class="text-truncate lh-lg"> <a href="/posts/k8s-commands/">Essential kubectl Commands</a><li class="text-truncate lh-lg"> <a href="/posts/nlp-basics/">NLP with Python: A Beginner's Guide</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/data-science/">data science</a> <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a> <a class="post-tag btn btn-outline-primary" href="/tags/cloud/">cloud</a> <a class="post-tag btn btn-outline-primary" href="/tags/feature-engineering/">feature engineering</a> <a class="post-tag btn btn-outline-primary" href="/tags/keras/">keras</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning/">machine learning</a> <a class="post-tag btn btn-outline-primary" href="/tags/pandas/">pandas</a> <a class="post-tag btn btn-outline-primary" href="/tags/scikit-learn/">scikit-learn</a> <a class="post-tag btn btn-outline-primary" href="/tags/tensorflow/">tensorflow</a> <a class="post-tag btn btn-outline-primary" href="/tags/devops/">devops</a></div></section></div><section id="toc-wrapper" class="ps-0 pe-4 mb-5"><h2 class="panel-heading ps-3 pt-2 mb-2">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/shap/" class="post-preview card h-100"><div class="card-body"> <time class="small" data-ts="1642255200" data-df="ll" > Jan 15, 2022 </time><h4 class="pt-0 my-2">Elucidating Machine Learning models with SHAP values</h4><div class="text-muted small"><p> The problem During data analysis, we may only look at metrics such as accuracy/precision/recall and worry about not overfitting our data. But are those the only ones that matter? When it comes to...</p></div></div></a></article><article class="col"> <a href="/posts/keras-101-boston/" class="post-preview card h-100"><div class="card-body"> <time class="small" data-ts="1644933600" data-df="ll" > Feb 15, 2022 </time><h4 class="pt-0 my-2">Keras 101: A simple Neural Network for House Pricing regression</h4><div class="text-muted small"><p> In this post, we will be covering some basics of data exploration and building a model with Keras in order to help us on predicting the selling price of a given house in the Boston (MA) area. As an...</p></div></div></a></article><article class="col"> <a href="/posts/keras-multi-output/" class="post-preview card h-100"><div class="card-body"> <time class="small" data-ts="1647352800" data-df="ll" > Mar 15, 2022 </time><h4 class="pt-0 my-2">Building a multi-output Convolutional Neural Network with Keras</h4><div class="text-muted small"><p> In this post, we will be exploring the Keras functional API in order to build a multi-output Deep Learning model. We will show how to train a single model that is capable of predicting three distin...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"><div class="btn btn-outline-primary disabled" aria-label="Older"><p>-</p></div><a href="/posts/shap/" class="btn btn-outline-primary" aria-label="Newer" ><p>Elucidating Machine Learning models with SHAP values</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p> © <time>2025</time> <a href="https://github.com/rodrigobressan">Rodrigo Bressan</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>See something wrong? Want to contribute? <a href="https://github.com/rodrigobressan/rodrigobressan.github.io/tree/master/_posts">Edit this page 📝</href></p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/data-science/">data science</a> <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a> <a class="post-tag btn btn-outline-primary" href="/tags/cloud/">cloud</a> <a class="post-tag btn btn-outline-primary" href="/tags/feature-engineering/">feature engineering</a> <a class="post-tag btn btn-outline-primary" href="/tags/keras/">keras</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning/">machine learning</a> <a class="post-tag btn btn-outline-primary" href="/tags/pandas/">pandas</a> <a class="post-tag btn btn-outline-primary" href="/tags/scikit-learn/">scikit-learn</a> <a class="post-tag btn btn-outline-primary" href="/tags/tensorflow/">tensorflow</a> <a class="post-tag btn btn-outline-primary" href="/tags/devops/">devops</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask"></div><script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.1/dist/jquery.min.js,npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.9/dayjs.min.js,npm/dayjs@1.11.9/locale/en.min.js,npm/dayjs@1.11.9/plugin/relativeTime.min.js,npm/dayjs@1.11.9/plugin/localizedFormat.min.js,npm/tocbot@4.21.1/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/unregister.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-5B9QC89SHN"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-5B9QC89SHN'); }); </script> <script> /* Note: dependent library will be loaded in `js-selector.html` */ SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5"></p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
