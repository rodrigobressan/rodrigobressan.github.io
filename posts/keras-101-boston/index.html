<!doctype html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Keras 101: A simple Neural Network for House Pricing regression" /><meta property="og:locale" content="en" /><meta name="description" content="In this post, we will be covering some basics of data exploration and building a model with Keras in order to help us on predicting the selling price of a given house in the Boston (MA) area. As an application of this model in the real world, you can think about being a real state agent looking for a tool to help you on your day-to-day duties, which for me, at least, sounds pretty good when compared to just gut-estimation. For this exercise, we will be using the Plotly library instead of the good ol’ fashioned matplotlib, due to having more interactive plots, which for sure help in understanding the data. We will also use the Scikit-Learn and Keras for building the models, Pandas library to manipulate our data and the SHAP library to generate explanations for our trained model. Importing the dataset In this example, we wil be using the sklearn.datasets module, which contains the Boston dataset. You could also use the keras.datasets module, but this one does not contain the labels of the features, so we decided to use scikits one. Let’s also convert it to a Pandas DataFrame and print it’s head. 1 2 3 4 5 6 7 8 9 from sklearn.datasets import load_boston import pandas as pd boston_dataset = load_boston() df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) df[&#39;MEDV&#39;] = boston_dataset.target df.head(n=10) This should output the following dataframe:   CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18 2.31 0 0.538 6.575 65.2 4.09 1 296 15.3 396.9 4.98 24 1 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.9 9.14 21.6 2 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 34.7 3 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 33.4 4 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.9 5.33 36.2 Exploratory Data Analysis Making yourself comfortable and familiar with your dataset is a fundamental step to help you comprehend your data and draw better conclusions and explanations from your results. Initially, let’s plot a few box plots, which will help us to better visualize anomalies and/or outliers in data distribution. If you are confused about what is a box plot and how it can help us to better visualizate the distribution of our data, here is a brief description from Ross (1977): In descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from plotly.subplots import make_subplots import plotly.graph_objects as go import math total_items = len(df.columns) items_per_row = 3 total_rows = math.ceil(total_items / items_per_row) fig = make_subplots(rows=total_rows, cols=items_per_row) cur_row = 1 cur_col = 1 for index, column in enumerate(df.columns): fig.add_trace(go.Box(y=df[column], name=column), row=cur_row, col=cur_col) if cur_col % items_per_row == 0: cur_col = 1 cur_row = cur_row + 1 else: cur_col = cur_col + 1 fig.update_layout(height=1000, width=550, showlegend=False) fig.show() You can hover on the subplots below to see the properties of each box plot. These results do corroborate our initial assumptions about having outliers in some columns. Let’s also plot some scatter plots for each feature and the target variable, as well as their intercept lines: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from plotly.subplots import make_subplots import plotly.graph_objects as go import math import numpy as np total_items = len(df.columns) items_per_row = 3 total_rows = math.ceil(total_items / items_per_row) fig = make_subplots(rows=total_rows, cols=items_per_row, subplot_titles=df.columns) cur_row = 1 cur_col = 1 for index, column in enumerate(df.columns): fig.add_trace(go.Scattergl(x=df[column], y=df[&#39;MEDV&#39;], mode=&quot;markers&quot;, marker=dict(size=3)), row=cur_row, col=cur_col) intercept = np.poly1d(np.polyfit(df[column], df[&#39;MEDV&#39;], 1))(np.unique(df[column])) fig.add_trace(go.Scatter(x=np.unique(df[column]), y=intercept, line=dict(color=&#39;red&#39;, width=1)), row=cur_row, col=cur_col) if cur_col % items_per_row == 0: cur_col = 1 cur_row = cur_row + 1 else: cur_col = cur_col + 1 fig.update_layout(height=1000, width=550, showlegend=False) fig.show() From this initial data exploration, we can have two major conclusions: There is a strong linear correlation between the RM (average number of rooms per dwelling) and LSTAT (% lower status of the population) with the target variable, being the RM a positive and the LSTAT a negative correlation. There are some records containing outliers, which we could preprocess in order to input our model with more normalized data. Data preprocessing Before we proceed into any data preprocessing, it’s important to split our data into training and test sets. We should not apply any kind of preprocessing into our data without taking into account that we should not leak information from our test set. For this step, we can use the train_test_split method from scikit-learn. In this case, we will use a split of 70% of the data for training and 30% for testing. We also set a random_state seed, in order to allow reprocibility. 1 2 3 4 5 from sklearn.model_selection import train_test_split X = df.loc[:, df.columns != &#39;MEDV&#39;] y = df.loc[:, df.columns == &#39;MEDV&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123) In order to provide a standardized input to our neural network, we need the perform the normalization of our dataset. This can be seen as an step to reduce the differences in scale that may arise from the existent features. We perform this normalization by subtracting the mean from our data and dividing it by the standard deviation. One more time, this normalization should only be performed by using the mean and standard deviation from the training set, in order to avoid any information leak from the test set. 1 2 3 4 5 mean = X_train.mean(axis=0) std = X_train.std(axis=0) X_train = (X_train - mean) / std X_test = (X_test - mean) / std Build our model Due to the small amount of presented data in this dataset, we must be careful to not create an overly complex model, which could lead to overfitting our data. For this, we are going to adopt an architecture based on two Dense layers, the first with 128 and the second with 64 neurons, both using a ReLU activation function. A dense layer with a linear activation will be used as output layer. In order to allow us to know if our model is properly learning, we will use a mean squared error loss function and to report the performance of it we will adopt the mean average error metric. By using the summary method from Keras, we can see that we have a total of 10,113 parameters, which is acceptable for us. 1 2 3 4 5 6 7 8 9 10 11 from keras.models import Sequential from keras.layers import Dense model = Sequential() model.add(Dense(128, input_shape=(13, ), activation=&#39;relu&#39;, name=&#39;dense_1&#39;)) model.add(Dense(64, activation=&#39;relu&#39;, name=&#39;dense_2&#39;)) model.add(Dense(1, activation=&#39;linear&#39;, name=&#39;dense_output&#39;)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;, metrics=[&#39;mae&#39;]) model.summary() Train our model This step is pretty straightforward: fit our model with both our features and their labels, for a total amount of 100 epochs, separating 5% of the samples (18 records) as validation set. 1 history = model.fit(X_train, y_train, epochs=100, validation_split=0.05) By plotting both loss and mean average error, we can see that our model was capable of learning patterns in our data without overfitting taking place (as shown by the validation set curves): 1 2 3 4 5 6 7 8 9 10 11 12 13 fig = go.Figure() fig.add_trace(go.Scattergl(y=history.history[&#39;loss&#39;], name=&#39;Train&#39;)) fig.add_trace(go.Scattergl(y=history.history[&#39;val_loss&#39;], name=&#39;Valid&#39;)) fig.update_layout(height=500, width=700, xaxis_title=&#39;Epoch&#39;, yaxis_title=&#39;Loss&#39;) fig.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 fig = go.Figure() fig.add_trace(go.Scattergl(y=history.history[&#39;mean_absolute_error&#39;], name=&#39;Train&#39;)) fig.add_trace(go.Scattergl(y=history.history[&#39;val_mean_absolute_error&#39;], name=&#39;Valid&#39;)) fig.update_layout(height=500, width=700, xaxis_title=&#39;Epoch&#39;, yaxis_title=&#39;Mean Absolute Error&#39;) fig.show() Evaluate our model 1 2 3 4 mse_nn, mae_nn = model.evaluate(X_test, y_test) print(&#39;Mean squared error on test data: &#39;, mse_nn) print(&#39;Mean absolute error on test data: &#39;, mae_nn) Output: 1 2 3 152/152 [==============================] - 0s 60us/step Mean squared error on test data: 17.429732523466413 Mean absolute error on test data: 2.6727954964888725 Comparison with traditional approaches First let’s try with a simple algorithm, the Linear Regression: 1 2 3 4 5 6 7 8 9 lr_model = LinearRegression() lr_model.fit(X_train, y_train) y_pred_lr = lr_model.predict(X_test) mse_lr = mean_squared_error(y_test, y_pred_lr) mae_lr = mean_absolute_error(y_test, y_pred_lr) print(&#39;Mean squared error on test data: &#39;, mse_lr) print(&#39;Mean absolute error on test data: &#39;, mae_lr) 1 2 Mean squared error on test data: 28.40585481050824 Mean absolute error on test data: 3.6913626771162575 And now with a Decision Tree: 1 2 3 4 5 6 7 8 9 10 tree = DecisionTreeRegressor() tree.fit(X_train, y_train) y_pred_tree = tree.predict(X_test) mse_dt = mean_squared_error(y_test, y_pred_tree) mae_dt = mean_absolute_error(y_test, y_pred_tree) print(&#39;Mean squared error on test data: &#39;, mse_dt) print(&#39;Mean absolute error on test data: &#39;, mae_dt) 1 2 Mean squared error on test data: 17.830657894736845 Mean absolute error on test data: 2.755263157894737 Opening the Black Box (a.k.a. Explaining our Model) Sometimes just a good result is enough for most of the people, but there are scenarios where we need to explain what are the major components used by our model to perform its prediction. For this task, we can rely on the SHAP library, which easily allows us to create a summary of our features and its impact on the model output. I won’t dive deep into the details of SHAP, but if you are intered on it, you can check their github page or even give a look at its paper]. 1 2 3 4 5 6 7 import shap shap.initjs() explainer = shap.DeepExplainer(model, X_train[:100].values) shap_values = explainer.shap_values(X_test[:100].values) shap.summary_plot(shap_values, X_test, plot_type=&#39;bar&#39;) From this simple plot, we can see that the major features that have an impact on the model output are: LSTAT: % lower status of the population RM: average number of rooms per dwelling RAD: index of accessibility to radial highways DIS: weighted distances to five Boston employment centres NOX: nitric oxides concentration (parts per 10 million) - this may more likely be correlated with greenness of the area CRIM: per capita crime rate by town From this plot, we can clearly corroborate our initial EDA analysis in which we point out the LSTAT and RM features as having a high correlation with the model outcome. Conclusions In this post, we have showed that by using a Neural Network, we can easily outperform traditional Machine Learning methods by a good margin. We also show that, even when using a more complex model, when compared to other techniques, we can still explain the outcomes of our model by using the SHAP library. Furthermore, we need to have in mind that the explored dataset can be somehow outdated, and some feature engineering (such as correcting prices for inflaction) could be performed in order to better reflect current scenarios. The Jupyter notebook for this post can be found here. References Boston Dataset: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html Plotly: https://plot.ly/python/ ScikitLearn: https://scikit-learn.org/stable/ Keras: https://keras.io/ Pandas: https://pandas.pydata.org/ SHAP Project Page: https://github.com/slundberg/shap SHAP Paper: https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf Introduction to probability and statistics for engineers and scientists. https://www.amazon.com.br/dp/B007ZBZN9U/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1" /><meta property="og:description" content="In this post, we will be covering some basics of data exploration and building a model with Keras in order to help us on predicting the selling price of a given house in the Boston (MA) area. As an application of this model in the real world, you can think about being a real state agent looking for a tool to help you on your day-to-day duties, which for me, at least, sounds pretty good when compared to just gut-estimation. For this exercise, we will be using the Plotly library instead of the good ol’ fashioned matplotlib, due to having more interactive plots, which for sure help in understanding the data. We will also use the Scikit-Learn and Keras for building the models, Pandas library to manipulate our data and the SHAP library to generate explanations for our trained model. Importing the dataset In this example, we wil be using the sklearn.datasets module, which contains the Boston dataset. You could also use the keras.datasets module, but this one does not contain the labels of the features, so we decided to use scikits one. Let’s also convert it to a Pandas DataFrame and print it’s head. 1 2 3 4 5 6 7 8 9 from sklearn.datasets import load_boston import pandas as pd boston_dataset = load_boston() df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) df[&#39;MEDV&#39;] = boston_dataset.target df.head(n=10) This should output the following dataframe:   CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18 2.31 0 0.538 6.575 65.2 4.09 1 296 15.3 396.9 4.98 24 1 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.9 9.14 21.6 2 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 34.7 3 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 33.4 4 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.9 5.33 36.2 Exploratory Data Analysis Making yourself comfortable and familiar with your dataset is a fundamental step to help you comprehend your data and draw better conclusions and explanations from your results. Initially, let’s plot a few box plots, which will help us to better visualize anomalies and/or outliers in data distribution. If you are confused about what is a box plot and how it can help us to better visualizate the distribution of our data, here is a brief description from Ross (1977): In descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from plotly.subplots import make_subplots import plotly.graph_objects as go import math total_items = len(df.columns) items_per_row = 3 total_rows = math.ceil(total_items / items_per_row) fig = make_subplots(rows=total_rows, cols=items_per_row) cur_row = 1 cur_col = 1 for index, column in enumerate(df.columns): fig.add_trace(go.Box(y=df[column], name=column), row=cur_row, col=cur_col) if cur_col % items_per_row == 0: cur_col = 1 cur_row = cur_row + 1 else: cur_col = cur_col + 1 fig.update_layout(height=1000, width=550, showlegend=False) fig.show() You can hover on the subplots below to see the properties of each box plot. These results do corroborate our initial assumptions about having outliers in some columns. Let’s also plot some scatter plots for each feature and the target variable, as well as their intercept lines: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from plotly.subplots import make_subplots import plotly.graph_objects as go import math import numpy as np total_items = len(df.columns) items_per_row = 3 total_rows = math.ceil(total_items / items_per_row) fig = make_subplots(rows=total_rows, cols=items_per_row, subplot_titles=df.columns) cur_row = 1 cur_col = 1 for index, column in enumerate(df.columns): fig.add_trace(go.Scattergl(x=df[column], y=df[&#39;MEDV&#39;], mode=&quot;markers&quot;, marker=dict(size=3)), row=cur_row, col=cur_col) intercept = np.poly1d(np.polyfit(df[column], df[&#39;MEDV&#39;], 1))(np.unique(df[column])) fig.add_trace(go.Scatter(x=np.unique(df[column]), y=intercept, line=dict(color=&#39;red&#39;, width=1)), row=cur_row, col=cur_col) if cur_col % items_per_row == 0: cur_col = 1 cur_row = cur_row + 1 else: cur_col = cur_col + 1 fig.update_layout(height=1000, width=550, showlegend=False) fig.show() From this initial data exploration, we can have two major conclusions: There is a strong linear correlation between the RM (average number of rooms per dwelling) and LSTAT (% lower status of the population) with the target variable, being the RM a positive and the LSTAT a negative correlation. There are some records containing outliers, which we could preprocess in order to input our model with more normalized data. Data preprocessing Before we proceed into any data preprocessing, it’s important to split our data into training and test sets. We should not apply any kind of preprocessing into our data without taking into account that we should not leak information from our test set. For this step, we can use the train_test_split method from scikit-learn. In this case, we will use a split of 70% of the data for training and 30% for testing. We also set a random_state seed, in order to allow reprocibility. 1 2 3 4 5 from sklearn.model_selection import train_test_split X = df.loc[:, df.columns != &#39;MEDV&#39;] y = df.loc[:, df.columns == &#39;MEDV&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123) In order to provide a standardized input to our neural network, we need the perform the normalization of our dataset. This can be seen as an step to reduce the differences in scale that may arise from the existent features. We perform this normalization by subtracting the mean from our data and dividing it by the standard deviation. One more time, this normalization should only be performed by using the mean and standard deviation from the training set, in order to avoid any information leak from the test set. 1 2 3 4 5 mean = X_train.mean(axis=0) std = X_train.std(axis=0) X_train = (X_train - mean) / std X_test = (X_test - mean) / std Build our model Due to the small amount of presented data in this dataset, we must be careful to not create an overly complex model, which could lead to overfitting our data. For this, we are going to adopt an architecture based on two Dense layers, the first with 128 and the second with 64 neurons, both using a ReLU activation function. A dense layer with a linear activation will be used as output layer. In order to allow us to know if our model is properly learning, we will use a mean squared error loss function and to report the performance of it we will adopt the mean average error metric. By using the summary method from Keras, we can see that we have a total of 10,113 parameters, which is acceptable for us. 1 2 3 4 5 6 7 8 9 10 11 from keras.models import Sequential from keras.layers import Dense model = Sequential() model.add(Dense(128, input_shape=(13, ), activation=&#39;relu&#39;, name=&#39;dense_1&#39;)) model.add(Dense(64, activation=&#39;relu&#39;, name=&#39;dense_2&#39;)) model.add(Dense(1, activation=&#39;linear&#39;, name=&#39;dense_output&#39;)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;, metrics=[&#39;mae&#39;]) model.summary() Train our model This step is pretty straightforward: fit our model with both our features and their labels, for a total amount of 100 epochs, separating 5% of the samples (18 records) as validation set. 1 history = model.fit(X_train, y_train, epochs=100, validation_split=0.05) By plotting both loss and mean average error, we can see that our model was capable of learning patterns in our data without overfitting taking place (as shown by the validation set curves): 1 2 3 4 5 6 7 8 9 10 11 12 13 fig = go.Figure() fig.add_trace(go.Scattergl(y=history.history[&#39;loss&#39;], name=&#39;Train&#39;)) fig.add_trace(go.Scattergl(y=history.history[&#39;val_loss&#39;], name=&#39;Valid&#39;)) fig.update_layout(height=500, width=700, xaxis_title=&#39;Epoch&#39;, yaxis_title=&#39;Loss&#39;) fig.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 fig = go.Figure() fig.add_trace(go.Scattergl(y=history.history[&#39;mean_absolute_error&#39;], name=&#39;Train&#39;)) fig.add_trace(go.Scattergl(y=history.history[&#39;val_mean_absolute_error&#39;], name=&#39;Valid&#39;)) fig.update_layout(height=500, width=700, xaxis_title=&#39;Epoch&#39;, yaxis_title=&#39;Mean Absolute Error&#39;) fig.show() Evaluate our model 1 2 3 4 mse_nn, mae_nn = model.evaluate(X_test, y_test) print(&#39;Mean squared error on test data: &#39;, mse_nn) print(&#39;Mean absolute error on test data: &#39;, mae_nn) Output: 1 2 3 152/152 [==============================] - 0s 60us/step Mean squared error on test data: 17.429732523466413 Mean absolute error on test data: 2.6727954964888725 Comparison with traditional approaches First let’s try with a simple algorithm, the Linear Regression: 1 2 3 4 5 6 7 8 9 lr_model = LinearRegression() lr_model.fit(X_train, y_train) y_pred_lr = lr_model.predict(X_test) mse_lr = mean_squared_error(y_test, y_pred_lr) mae_lr = mean_absolute_error(y_test, y_pred_lr) print(&#39;Mean squared error on test data: &#39;, mse_lr) print(&#39;Mean absolute error on test data: &#39;, mae_lr) 1 2 Mean squared error on test data: 28.40585481050824 Mean absolute error on test data: 3.6913626771162575 And now with a Decision Tree: 1 2 3 4 5 6 7 8 9 10 tree = DecisionTreeRegressor() tree.fit(X_train, y_train) y_pred_tree = tree.predict(X_test) mse_dt = mean_squared_error(y_test, y_pred_tree) mae_dt = mean_absolute_error(y_test, y_pred_tree) print(&#39;Mean squared error on test data: &#39;, mse_dt) print(&#39;Mean absolute error on test data: &#39;, mae_dt) 1 2 Mean squared error on test data: 17.830657894736845 Mean absolute error on test data: 2.755263157894737 Opening the Black Box (a.k.a. Explaining our Model) Sometimes just a good result is enough for most of the people, but there are scenarios where we need to explain what are the major components used by our model to perform its prediction. For this task, we can rely on the SHAP library, which easily allows us to create a summary of our features and its impact on the model output. I won’t dive deep into the details of SHAP, but if you are intered on it, you can check their github page or even give a look at its paper]. 1 2 3 4 5 6 7 import shap shap.initjs() explainer = shap.DeepExplainer(model, X_train[:100].values) shap_values = explainer.shap_values(X_test[:100].values) shap.summary_plot(shap_values, X_test, plot_type=&#39;bar&#39;) From this simple plot, we can see that the major features that have an impact on the model output are: LSTAT: % lower status of the population RM: average number of rooms per dwelling RAD: index of accessibility to radial highways DIS: weighted distances to five Boston employment centres NOX: nitric oxides concentration (parts per 10 million) - this may more likely be correlated with greenness of the area CRIM: per capita crime rate by town From this plot, we can clearly corroborate our initial EDA analysis in which we point out the LSTAT and RM features as having a high correlation with the model outcome. Conclusions In this post, we have showed that by using a Neural Network, we can easily outperform traditional Machine Learning methods by a good margin. We also show that, even when using a more complex model, when compared to other techniques, we can still explain the outcomes of our model by using the SHAP library. Furthermore, we need to have in mind that the explored dataset can be somehow outdated, and some feature engineering (such as correcting prices for inflaction) could be performed in order to better reflect current scenarios. The Jupyter notebook for this post can be found here. References Boston Dataset: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html Plotly: https://plot.ly/python/ ScikitLearn: https://scikit-learn.org/stable/ Keras: https://keras.io/ Pandas: https://pandas.pydata.org/ SHAP Project Page: https://github.com/slundberg/shap SHAP Paper: https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf Introduction to probability and statistics for engineers and scientists. https://www.amazon.com.br/dp/B007ZBZN9U/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1" /><link rel="canonical" href="https://bressan.xyz/posts/keras-101-boston/" /><meta property="og:url" content="https://bressan.xyz/posts/keras-101-boston/" /><meta property="og:site_name" content="Rodrigo Bressan" /><meta property="og:image" content="https://bressan.xyz/assets/img/posts/03-keras/banner.jpeg" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-02-15T15:00:00+01:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://bressan.xyz/assets/img/posts/03-keras/banner.jpeg" /><meta property="twitter:title" content="Keras 101: A simple Neural Network for House Pricing regression" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-02-15T15:00:00+01:00","datePublished":"2022-02-15T15:00:00+01:00","description":"In this post, we will be covering some basics of data exploration and building a model with Keras in order to help us on predicting the selling price of a given house in the Boston (MA) area. As an application of this model in the real world, you can think about being a real state agent looking for a tool to help you on your day-to-day duties, which for me, at least, sounds pretty good when compared to just gut-estimation. For this exercise, we will be using the Plotly library instead of the good ol’ fashioned matplotlib, due to having more interactive plots, which for sure help in understanding the data. We will also use the Scikit-Learn and Keras for building the models, Pandas library to manipulate our data and the SHAP library to generate explanations for our trained model. Importing the dataset In this example, we wil be using the sklearn.datasets module, which contains the Boston dataset. You could also use the keras.datasets module, but this one does not contain the labels of the features, so we decided to use scikits one. Let’s also convert it to a Pandas DataFrame and print it’s head. 1 2 3 4 5 6 7 8 9 from sklearn.datasets import load_boston import pandas as pd boston_dataset = load_boston() df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) df[&#39;MEDV&#39;] = boston_dataset.target df.head(n=10) This should output the following dataframe:   CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18 2.31 0 0.538 6.575 65.2 4.09 1 296 15.3 396.9 4.98 24 1 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.9 9.14 21.6 2 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 34.7 3 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 33.4 4 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.9 5.33 36.2 Exploratory Data Analysis Making yourself comfortable and familiar with your dataset is a fundamental step to help you comprehend your data and draw better conclusions and explanations from your results. Initially, let’s plot a few box plots, which will help us to better visualize anomalies and/or outliers in data distribution. If you are confused about what is a box plot and how it can help us to better visualizate the distribution of our data, here is a brief description from Ross (1977): In descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from plotly.subplots import make_subplots import plotly.graph_objects as go import math total_items = len(df.columns) items_per_row = 3 total_rows = math.ceil(total_items / items_per_row) fig = make_subplots(rows=total_rows, cols=items_per_row) cur_row = 1 cur_col = 1 for index, column in enumerate(df.columns): fig.add_trace(go.Box(y=df[column], name=column), row=cur_row, col=cur_col) if cur_col % items_per_row == 0: cur_col = 1 cur_row = cur_row + 1 else: cur_col = cur_col + 1 fig.update_layout(height=1000, width=550, showlegend=False) fig.show() You can hover on the subplots below to see the properties of each box plot. These results do corroborate our initial assumptions about having outliers in some columns. Let’s also plot some scatter plots for each feature and the target variable, as well as their intercept lines: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from plotly.subplots import make_subplots import plotly.graph_objects as go import math import numpy as np total_items = len(df.columns) items_per_row = 3 total_rows = math.ceil(total_items / items_per_row) fig = make_subplots(rows=total_rows, cols=items_per_row, subplot_titles=df.columns) cur_row = 1 cur_col = 1 for index, column in enumerate(df.columns): fig.add_trace(go.Scattergl(x=df[column], y=df[&#39;MEDV&#39;], mode=&quot;markers&quot;, marker=dict(size=3)), row=cur_row, col=cur_col) intercept = np.poly1d(np.polyfit(df[column], df[&#39;MEDV&#39;], 1))(np.unique(df[column])) fig.add_trace(go.Scatter(x=np.unique(df[column]), y=intercept, line=dict(color=&#39;red&#39;, width=1)), row=cur_row, col=cur_col) if cur_col % items_per_row == 0: cur_col = 1 cur_row = cur_row + 1 else: cur_col = cur_col + 1 fig.update_layout(height=1000, width=550, showlegend=False) fig.show() From this initial data exploration, we can have two major conclusions: There is a strong linear correlation between the RM (average number of rooms per dwelling) and LSTAT (% lower status of the population) with the target variable, being the RM a positive and the LSTAT a negative correlation. There are some records containing outliers, which we could preprocess in order to input our model with more normalized data. Data preprocessing Before we proceed into any data preprocessing, it’s important to split our data into training and test sets. We should not apply any kind of preprocessing into our data without taking into account that we should not leak information from our test set. For this step, we can use the train_test_split method from scikit-learn. In this case, we will use a split of 70% of the data for training and 30% for testing. We also set a random_state seed, in order to allow reprocibility. 1 2 3 4 5 from sklearn.model_selection import train_test_split X = df.loc[:, df.columns != &#39;MEDV&#39;] y = df.loc[:, df.columns == &#39;MEDV&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123) In order to provide a standardized input to our neural network, we need the perform the normalization of our dataset. This can be seen as an step to reduce the differences in scale that may arise from the existent features. We perform this normalization by subtracting the mean from our data and dividing it by the standard deviation. One more time, this normalization should only be performed by using the mean and standard deviation from the training set, in order to avoid any information leak from the test set. 1 2 3 4 5 mean = X_train.mean(axis=0) std = X_train.std(axis=0) X_train = (X_train - mean) / std X_test = (X_test - mean) / std Build our model Due to the small amount of presented data in this dataset, we must be careful to not create an overly complex model, which could lead to overfitting our data. For this, we are going to adopt an architecture based on two Dense layers, the first with 128 and the second with 64 neurons, both using a ReLU activation function. A dense layer with a linear activation will be used as output layer. In order to allow us to know if our model is properly learning, we will use a mean squared error loss function and to report the performance of it we will adopt the mean average error metric. By using the summary method from Keras, we can see that we have a total of 10,113 parameters, which is acceptable for us. 1 2 3 4 5 6 7 8 9 10 11 from keras.models import Sequential from keras.layers import Dense model = Sequential() model.add(Dense(128, input_shape=(13, ), activation=&#39;relu&#39;, name=&#39;dense_1&#39;)) model.add(Dense(64, activation=&#39;relu&#39;, name=&#39;dense_2&#39;)) model.add(Dense(1, activation=&#39;linear&#39;, name=&#39;dense_output&#39;)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;, metrics=[&#39;mae&#39;]) model.summary() Train our model This step is pretty straightforward: fit our model with both our features and their labels, for a total amount of 100 epochs, separating 5% of the samples (18 records) as validation set. 1 history = model.fit(X_train, y_train, epochs=100, validation_split=0.05) By plotting both loss and mean average error, we can see that our model was capable of learning patterns in our data without overfitting taking place (as shown by the validation set curves): 1 2 3 4 5 6 7 8 9 10 11 12 13 fig = go.Figure() fig.add_trace(go.Scattergl(y=history.history[&#39;loss&#39;], name=&#39;Train&#39;)) fig.add_trace(go.Scattergl(y=history.history[&#39;val_loss&#39;], name=&#39;Valid&#39;)) fig.update_layout(height=500, width=700, xaxis_title=&#39;Epoch&#39;, yaxis_title=&#39;Loss&#39;) fig.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 fig = go.Figure() fig.add_trace(go.Scattergl(y=history.history[&#39;mean_absolute_error&#39;], name=&#39;Train&#39;)) fig.add_trace(go.Scattergl(y=history.history[&#39;val_mean_absolute_error&#39;], name=&#39;Valid&#39;)) fig.update_layout(height=500, width=700, xaxis_title=&#39;Epoch&#39;, yaxis_title=&#39;Mean Absolute Error&#39;) fig.show() Evaluate our model 1 2 3 4 mse_nn, mae_nn = model.evaluate(X_test, y_test) print(&#39;Mean squared error on test data: &#39;, mse_nn) print(&#39;Mean absolute error on test data: &#39;, mae_nn) Output: 1 2 3 152/152 [==============================] - 0s 60us/step Mean squared error on test data: 17.429732523466413 Mean absolute error on test data: 2.6727954964888725 Comparison with traditional approaches First let’s try with a simple algorithm, the Linear Regression: 1 2 3 4 5 6 7 8 9 lr_model = LinearRegression() lr_model.fit(X_train, y_train) y_pred_lr = lr_model.predict(X_test) mse_lr = mean_squared_error(y_test, y_pred_lr) mae_lr = mean_absolute_error(y_test, y_pred_lr) print(&#39;Mean squared error on test data: &#39;, mse_lr) print(&#39;Mean absolute error on test data: &#39;, mae_lr) 1 2 Mean squared error on test data: 28.40585481050824 Mean absolute error on test data: 3.6913626771162575 And now with a Decision Tree: 1 2 3 4 5 6 7 8 9 10 tree = DecisionTreeRegressor() tree.fit(X_train, y_train) y_pred_tree = tree.predict(X_test) mse_dt = mean_squared_error(y_test, y_pred_tree) mae_dt = mean_absolute_error(y_test, y_pred_tree) print(&#39;Mean squared error on test data: &#39;, mse_dt) print(&#39;Mean absolute error on test data: &#39;, mae_dt) 1 2 Mean squared error on test data: 17.830657894736845 Mean absolute error on test data: 2.755263157894737 Opening the Black Box (a.k.a. Explaining our Model) Sometimes just a good result is enough for most of the people, but there are scenarios where we need to explain what are the major components used by our model to perform its prediction. For this task, we can rely on the SHAP library, which easily allows us to create a summary of our features and its impact on the model output. I won’t dive deep into the details of SHAP, but if you are intered on it, you can check their github page or even give a look at its paper]. 1 2 3 4 5 6 7 import shap shap.initjs() explainer = shap.DeepExplainer(model, X_train[:100].values) shap_values = explainer.shap_values(X_test[:100].values) shap.summary_plot(shap_values, X_test, plot_type=&#39;bar&#39;) From this simple plot, we can see that the major features that have an impact on the model output are: LSTAT: % lower status of the population RM: average number of rooms per dwelling RAD: index of accessibility to radial highways DIS: weighted distances to five Boston employment centres NOX: nitric oxides concentration (parts per 10 million) - this may more likely be correlated with greenness of the area CRIM: per capita crime rate by town From this plot, we can clearly corroborate our initial EDA analysis in which we point out the LSTAT and RM features as having a high correlation with the model outcome. Conclusions In this post, we have showed that by using a Neural Network, we can easily outperform traditional Machine Learning methods by a good margin. We also show that, even when using a more complex model, when compared to other techniques, we can still explain the outcomes of our model by using the SHAP library. Furthermore, we need to have in mind that the explored dataset can be somehow outdated, and some feature engineering (such as correcting prices for inflaction) could be performed in order to better reflect current scenarios. The Jupyter notebook for this post can be found here. References Boston Dataset: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html Plotly: https://plot.ly/python/ ScikitLearn: https://scikit-learn.org/stable/ Keras: https://keras.io/ Pandas: https://pandas.pydata.org/ SHAP Project Page: https://github.com/slundberg/shap SHAP Paper: https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf Introduction to probability and statistics for engineers and scientists. https://www.amazon.com.br/dp/B007ZBZN9U/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1","headline":"Keras 101: A simple Neural Network for House Pricing regression","image":"https://bressan.xyz/assets/img/posts/03-keras/banner.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"https://bressan.xyz/posts/keras-101-boston/"},"url":"https://bressan.xyz/posts/keras-101-boston/"}</script><title>Keras 101: A simple Neural Network for House Pricing regression | Rodrigo Bressan</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Rodrigo Bressan"><meta name="application-name" content="Rodrigo Bressan"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.1/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"> <img src="/assets/img/me.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"> </a><h1 class="site-title"> <a href="/">Rodrigo Bressan</a></h1><p class="site-subtitle fst-italic mb-0">Software, Healthcare and Improving People's lives</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a><li class="nav-item"> <a href="/contact/" class="nav-link"> <i class="fa-fw fas fa-address-card"></i> <span>CONTACT</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fas fa-code"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/resume/" class="nav-link"> <i class="fa-fw fas fa-file"></i> <span>RESUME</span> </a><li class="nav-item"> <a href="/all-posts/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ALL POSTS</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <a href="https://github.com/rodrigobressan" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://www.linkedin.com/in/rbressan/" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container px-xxl-5"><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Keras 101: A simple Neural Network for House Pricing regression</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4" ><article class="px-1"><header><h1 data-toc-skip>Keras 101: A simple Neural Network for House Pricing regression</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1644933600" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Feb 15, 2022 </time> </span><div class="mt-3 mb-3"> <a href="/assets/img/posts/03-keras/banner.jpeg" class="popup img-link preview-img shimmer"><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1200 630'%3E%3C/svg%3E" data-src="/assets/img/posts/03-keras/banner.jpeg" alt="Preview Image" width="1200" height="630" class="lazyload" data-proofer-ignore></a></div><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/rodrigobressan">Rodrigo Bressan</a> </em> </span> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="1851 words" > <em>10 min</em> read</span></div></div></header><div class="content"><p>In this post, we will be covering some basics of data exploration and building a model with Keras in order to help us on predicting the selling price of a given house in the Boston (MA) area. As an application of this model in the real world, you can think about being a real state agent looking for a tool to help you on your day-to-day duties, which for me, at least, sounds pretty good when compared to just gut-estimation.</p><p>For this exercise, we will be using the <a href="https://plot.ly/python/">Plotly</a> library instead of the good ol’ fashioned matplotlib, due to having more interactive plots, which for sure help in understanding the data. We will also use the <a href="https://scikit-learn.org/stable/">Scikit-Learn</a> and <a href="https://keras.io/">Keras</a> for building the models, <a href="https://pandas.pydata.org/">Pandas</a> library to manipulate our data and the <a href="https://github.com/slundberg/shap">SHAP library</a> to generate explanations for our trained model.</p><h2 id="importing-the-dataset"><span class="me-2">Importing the dataset</span><a href="#importing-the-dataset" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>In this example, we wil be using the sklearn.datasets module, which contains the Boston dataset. You could also use the keras.datasets module, but this one does not contain the labels of the features, so we decided to use scikits one. Let’s also convert it to a Pandas DataFrame and print it’s head.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">boston_dataset</span> <span class="o">=</span> <span class="nf">load_boston</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">boston_dataset</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">boston_dataset</span><span class="p">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">MEDV</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">boston_dataset</span><span class="p">.</span><span class="n">target</span>

<span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></table></code></div></div><p>This should output the following dataframe:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: right"> <th style="text-align: right">CRIM<th style="text-align: right">ZN<th style="text-align: right">INDUS<th style="text-align: right">CHAS<th style="text-align: right">NOX<th style="text-align: right">RM<th style="text-align: right">AGE<th style="text-align: right">DIS<th style="text-align: right">RAD<th style="text-align: right">TAX<th style="text-align: right">PTRATIO<th style="text-align: right">B<th style="text-align: right">LSTAT<th style="text-align: right">MEDV<tbody><tr><td style="text-align: right">0<td style="text-align: right">0.00632<td style="text-align: right">18<td style="text-align: right">2.31<td style="text-align: right">0<td style="text-align: right">0.538<td style="text-align: right">6.575<td style="text-align: right">65.2<td style="text-align: right">4.09<td style="text-align: right">1<td style="text-align: right">296<td style="text-align: right">15.3<td style="text-align: right">396.9<td style="text-align: right">4.98<td style="text-align: right">24<tr><td style="text-align: right">1<td style="text-align: right">0.02731<td style="text-align: right">0<td style="text-align: right">7.07<td style="text-align: right">0<td style="text-align: right">0.469<td style="text-align: right">6.421<td style="text-align: right">78.9<td style="text-align: right">4.9671<td style="text-align: right">2<td style="text-align: right">242<td style="text-align: right">17.8<td style="text-align: right">396.9<td style="text-align: right">9.14<td style="text-align: right">21.6<tr><td style="text-align: right">2<td style="text-align: right">0.02729<td style="text-align: right">0<td style="text-align: right">7.07<td style="text-align: right">0<td style="text-align: right">0.469<td style="text-align: right">7.185<td style="text-align: right">61.1<td style="text-align: right">4.9671<td style="text-align: right">2<td style="text-align: right">242<td style="text-align: right">17.8<td style="text-align: right">392.83<td style="text-align: right">4.03<td style="text-align: right">34.7<tr><td style="text-align: right">3<td style="text-align: right">0.03237<td style="text-align: right">0<td style="text-align: right">2.18<td style="text-align: right">0<td style="text-align: right">0.458<td style="text-align: right">6.998<td style="text-align: right">45.8<td style="text-align: right">6.0622<td style="text-align: right">3<td style="text-align: right">222<td style="text-align: right">18.7<td style="text-align: right">394.63<td style="text-align: right">2.94<td style="text-align: right">33.4<tr><td style="text-align: right">4<td style="text-align: right">0.06905<td style="text-align: right">0<td style="text-align: right">2.18<td style="text-align: right">0<td style="text-align: right">0.458<td style="text-align: right">7.147<td style="text-align: right">54.2<td style="text-align: right">6.0622<td style="text-align: right">3<td style="text-align: right">222<td style="text-align: right">18.7<td style="text-align: right">396.9<td style="text-align: right">5.33<td style="text-align: right">36.2</table></div><h2 id="exploratory-data-analysis"><span class="me-2">Exploratory Data Analysis</span><a href="#exploratory-data-analysis" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Making yourself comfortable and familiar with your dataset is a fundamental step to help you comprehend your data and draw better conclusions and explanations from your results.</p><p>Initially, let’s plot a few box plots, which will help us to better visualize anomalies and/or outliers in data distribution. If you are confused about what is a box plot and how it can help us to better visualizate the distribution of our data, here is a brief description from Ross (1977):</p><blockquote><p>In descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.</p></blockquote><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">plotly.subplots</span> <span class="kn">import</span> <span class="n">make_subplots</span>
<span class="kn">import</span> <span class="n">plotly.graph_objects</span> <span class="k">as</span> <span class="n">go</span>
<span class="kn">import</span> <span class="n">math</span>

<span class="n">total_items</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">items_per_row</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">total_rows</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">ceil</span><span class="p">(</span><span class="n">total_items</span> <span class="o">/</span> <span class="n">items_per_row</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="nf">make_subplots</span><span class="p">(</span><span class="n">rows</span><span class="o">=</span><span class="n">total_rows</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="n">items_per_row</span><span class="p">)</span>

<span class="n">cur_row</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">cur_col</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">column</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">go</span><span class="p">.</span><span class="nc">Box</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">column</span><span class="p">),</span> <span class="n">row</span><span class="o">=</span><span class="n">cur_row</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">cur_col</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">cur_col</span> <span class="o">%</span> <span class="n">items_per_row</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">cur_col</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">cur_row</span> <span class="o">=</span> <span class="n">cur_row</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cur_col</span> <span class="o">=</span> <span class="n">cur_col</span> <span class="o">+</span> <span class="mi">1</span>
    

<span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">550</span><span class="p">,</span>  <span class="n">showlegend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></table></code></div></div><p>You can hover on the subplots below to see the properties of each box plot.</p><iframe src="/assets/plots/03-keras/box-plot.html" id="igraph" scrolling="no" style="border:none;" seamless="seamless" height="1000" width="100%" loading="lazy"></iframe><p>These results do corroborate our initial assumptions about having outliers in some columns. Let’s also plot some scatter plots for each feature and the target variable, as well as their intercept lines:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">plotly.subplots</span> <span class="kn">import</span> <span class="n">make_subplots</span>
<span class="kn">import</span> <span class="n">plotly.graph_objects</span> <span class="k">as</span> <span class="n">go</span>
<span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">total_items</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">items_per_row</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">total_rows</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">ceil</span><span class="p">(</span><span class="n">total_items</span> <span class="o">/</span> <span class="n">items_per_row</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="nf">make_subplots</span><span class="p">(</span><span class="n">rows</span><span class="o">=</span><span class="n">total_rows</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="n">items_per_row</span><span class="p">,</span> <span class="n">subplot_titles</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>

<span class="n">cur_row</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">cur_col</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">column</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">go</span><span class="p">.</span><span class="nc">Scattergl</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">],</span> 
                            <span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">MEDV</span><span class="sh">'</span><span class="p">],</span> 
                            <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">markers</span><span class="sh">"</span><span class="p">,</span> 
                            <span class="n">marker</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)),</span> 
                  <span class="n">row</span><span class="o">=</span><span class="n">cur_row</span><span class="p">,</span> 
                  <span class="n">col</span><span class="o">=</span><span class="n">cur_col</span><span class="p">)</span>
    
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">poly1d</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">polyfit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">MEDV</span><span class="sh">'</span><span class="p">],</span> <span class="mi">1</span><span class="p">))(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]))</span>
    
    <span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">go</span><span class="p">.</span><span class="nc">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]),</span> 
                             <span class="n">y</span><span class="o">=</span><span class="n">intercept</span><span class="p">,</span> 
                             <span class="n">line</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span> 
                  <span class="n">row</span><span class="o">=</span><span class="n">cur_row</span><span class="p">,</span> 
                  <span class="n">col</span><span class="o">=</span><span class="n">cur_col</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">cur_col</span> <span class="o">%</span> <span class="n">items_per_row</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">cur_col</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">cur_row</span> <span class="o">=</span> <span class="n">cur_row</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cur_col</span> <span class="o">=</span> <span class="n">cur_col</span> <span class="o">+</span> <span class="mi">1</span>
    

<span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">550</span><span class="p">,</span> <span class="n">showlegend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></table></code></div></div><div style="width: 100%; text-align: center"> <a href="/assets/img/posts/03-keras/scatter.png" class="popup img-link "><img style="width: 100%; object-fit: contain" data-src="/assets/img/posts/03-keras/scatter.png" class="lazyload" data-proofer-ignore></a></div><p>From this initial data exploration, we can have two major conclusions:</p><ul><li>There is a strong linear correlation between the RM (average number of rooms per dwelling) and LSTAT (% lower status of the population) with the target variable, being the RM a positive and the LSTAT a negative correlation.<li>There are some records containing outliers, which we could preprocess in order to input our model with more normalized data.</ul><h2 id="data-preprocessing"><span class="me-2">Data preprocessing</span><a href="#data-preprocessing" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Before we proceed into any data preprocessing, it’s important to split our data into training and test sets. We should not apply any kind of preprocessing into our data without taking into account that we should not leak information from our test set. For this step, we can use the <em>train_test_split</em> method from scikit-learn. In this case, we will use a split of 70% of the data for training and 30% for testing. We also set a random_state seed, in order to allow reprocibility.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="sh">'</span><span class="s">MEDV</span><span class="sh">'</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span> <span class="o">==</span> <span class="sh">'</span><span class="s">MEDV</span><span class="sh">'</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</pre></table></code></div></div><p>In order to provide a standardized input to our neural network, we need the perform the normalization of our dataset. This can be seen as an step to reduce the differences in scale that may arise from the existent features. We perform this normalization by subtracting the mean from our data and dividing it by the standard deviation. <strong>One more time, this normalization should only be performed by using the mean and standard deviation from the training set, in order to avoid any information leak from the test set.</strong></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">mean</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
</pre></table></code></div></div><h2 id="build-our-model"><span class="me-2">Build our model</span><a href="#build-our-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Due to the small amount of presented data in this dataset, we must be careful to not create an overly complex model, which could lead to overfitting our data. For this, we are going to adopt an architecture based on two Dense layers, the first with 128 and the second with 64 neurons, both using a ReLU activation function. A dense layer with a linear activation will be used as output layer.</p><p>In order to allow us to know if our model is properly learning, we will use a mean squared error loss function and to report the performance of it we will adopt the mean average error metric.</p><p>By using the summary method from Keras, we can see that we have a total of 10,113 parameters, which is acceptable for us.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">dense_1</span><span class="sh">'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">dense_2</span><span class="sh">'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">dense_output</span><span class="sh">'</span><span class="p">))</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">mse</span><span class="sh">'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">mae</span><span class="sh">'</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</pre></table></code></div></div><h2 id="train-our-model"><span class="me-2">Train our model</span><a href="#train-our-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>This step is pretty straightforward: fit our model with both our features and their labels, for a total amount of 100 epochs, separating 5% of the samples (18 records) as validation set.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre></table></code></div></div><p>By plotting both loss and mean average error, we can see that our model was capable of learning patterns in our data without overfitting taking place (as shown by the validation set curves):</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="p">.</span><span class="nc">Figure</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">go</span><span class="p">.</span><span class="nc">Scattergl</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">],</span>
                    <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">Train</span><span class="sh">'</span><span class="p">))</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">go</span><span class="p">.</span><span class="nc">Scattergl</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">val_loss</span><span class="sh">'</span><span class="p">],</span>
                    <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">Valid</span><span class="sh">'</span><span class="p">))</span>


<span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">700</span><span class="p">,</span>
                  <span class="n">xaxis_title</span><span class="o">=</span><span class="sh">'</span><span class="s">Epoch</span><span class="sh">'</span><span class="p">,</span>
                  <span class="n">yaxis_title</span><span class="o">=</span><span class="sh">'</span><span class="s">Loss</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></table></code></div></div><iframe src="/assets/plots/03-keras/loss.html" id="igraph" scrolling="no" style="border:none;" seamless="seamless" height="450" width="100%" loading="lazy"></iframe><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="p">.</span><span class="nc">Figure</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">go</span><span class="p">.</span><span class="nc">Scattergl</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">mean_absolute_error</span><span class="sh">'</span><span class="p">],</span>
                    <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">Train</span><span class="sh">'</span><span class="p">))</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">go</span><span class="p">.</span><span class="nc">Scattergl</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">val_mean_absolute_error</span><span class="sh">'</span><span class="p">],</span>
                    <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">Valid</span><span class="sh">'</span><span class="p">))</span>


<span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">700</span><span class="p">,</span>
                  <span class="n">xaxis_title</span><span class="o">=</span><span class="sh">'</span><span class="s">Epoch</span><span class="sh">'</span><span class="p">,</span>
                  <span class="n">yaxis_title</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Absolute Error</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</pre></table></code></div></div><iframe src="/assets/plots/03-keras/mae.html" id="igraph" scrolling="no" style="border:none;" seamless="seamless" height="450" width="100%" loading="lazy"></iframe><h2 id="evaluate-our-model"><span class="me-2">Evaluate our model</span><a href="#evaluate-our-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">mse_nn</span><span class="p">,</span> <span class="n">mae_nn</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Mean squared error on test data: </span><span class="sh">'</span><span class="p">,</span> <span class="n">mse_nn</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Mean absolute error on test data: </span><span class="sh">'</span><span class="p">,</span> <span class="n">mae_nn</span><span class="p">)</span>
</pre></table></code></div></div><p>Output:</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>152/152 [==============================] - 0s 60us/step
Mean squared error on test data:  17.429732523466413
Mean absolute error on test data:  2.6727954964888725
</pre></table></code></div></div><h2 id="comparison-with-traditional-approaches"><span class="me-2">Comparison with traditional approaches</span><a href="#comparison-with-traditional-approaches" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>First let’s try with a simple algorithm, the Linear Regression:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="n">lr_model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">lr_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_lr</span> <span class="o">=</span> <span class="n">lr_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mse_lr</span> <span class="o">=</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">)</span>
<span class="n">mae_lr</span> <span class="o">=</span> <span class="nf">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Mean squared error on test data: </span><span class="sh">'</span><span class="p">,</span> <span class="n">mse_lr</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Mean absolute error on test data: </span><span class="sh">'</span><span class="p">,</span> <span class="n">mae_lr</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>Mean squared error on test data: 28.40585481050824
Mean absolute error on test data: 3.6913626771162575
</pre></table></code></div></div><p>And now with a Decision Tree:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="n">tree</span> <span class="o">=</span> <span class="nc">DecisionTreeRegressor</span><span class="p">()</span>
<span class="n">tree</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">mse_dt</span> <span class="o">=</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_tree</span><span class="p">)</span>
<span class="n">mae_dt</span> <span class="o">=</span> <span class="nf">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_tree</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Mean squared error on test data: </span><span class="sh">'</span><span class="p">,</span> <span class="n">mse_dt</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Mean absolute error on test data: </span><span class="sh">'</span><span class="p">,</span> <span class="n">mae_dt</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>Mean squared error on test data:  17.830657894736845
Mean absolute error on test data:  2.755263157894737
</pre></table></code></div></div><h2 id="opening-the-black-box-aka-explaining-our-model"><span class="me-2">Opening the Black Box (a.k.a. Explaining our Model)</span><a href="#opening-the-black-box-aka-explaining-our-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Sometimes just a good result is enough for most of the people, but there are scenarios where we need to explain what are the major components used by our model to perform its prediction. For this task, we can rely on the SHAP library, which easily allows us to create a summary of our features and its impact on the model output. I won’t dive deep into the details of SHAP, but if you are intered on it, you can check their <a href="https://github.com/slundberg/shap">github page</a> or even give a look at its <a href="https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf">paper</a>].</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed=""><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">shap</span>
<span class="n">shap</span><span class="p">.</span><span class="nf">initjs</span><span class="p">()</span>

<span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="p">.</span><span class="nc">DeepExplainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">100</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">.</span><span class="nf">shap_values</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">100</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>

<span class="n">shap</span><span class="p">.</span><span class="nf">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">plot_type</span><span class="o">=</span><span class="sh">'</span><span class="s">bar</span><span class="sh">'</span><span class="p">)</span>
</pre></table></code></div></div><div style="width: 100%; text-align: center"> <a href="/assets/img/posts/03-keras/shap.png" class="popup img-link "><img style="width: 70%; object-fit: contain" data-src="/assets/img/posts/03-keras/shap.png" class="lazyload" data-proofer-ignore></a></div><p>From this simple plot, we can see that the major features that have an impact on the model output are:</p><ul><li>LSTAT: % lower status of the population<li>RM: average number of rooms per dwelling<li>RAD: index of accessibility to radial highways<li>DIS: weighted distances to five Boston employment centres<li>NOX: nitric oxides concentration (parts per 10 million) - this may more likely be correlated with greenness of the area<li>CRIM: per capita crime rate by town</ul><p>From this plot, we can clearly corroborate our initial EDA analysis in which we point out the LSTAT and RM features as having a high correlation with the model outcome.</p><h2 id="conclusions"><span class="me-2">Conclusions</span><a href="#conclusions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>In this post, we have showed that by using a Neural Network, we can easily outperform traditional Machine Learning methods by a good margin. We also show that, even when using a more complex model, when compared to other techniques, we can still explain the outcomes of our model by using the SHAP library.</p><p>Furthermore, we need to have in mind that the explored dataset can be somehow outdated, and some feature engineering (such as correcting prices for inflaction) could be performed in order to better reflect current scenarios.</p><p>The Jupyter notebook for this post can be found <a href="/notebooks/03_cnn_boston.ipynb">here</a>.</p><h2 id="references"><span class="me-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Boston Dataset: <a href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html">https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html</a></p><p>Plotly: <a href="https://plot.ly/python/">https://plot.ly/python/</a></p><p>ScikitLearn: <a href="https://scikit-learn.org/stable/">https://scikit-learn.org/stable/</a></p><p>Keras: <a href="https://keras.io/">https://keras.io/</a></p><p>Pandas: <a href="https://pandas.pydata.org/">https://pandas.pydata.org/</a></p><p>SHAP Project Page: <a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></p><p>SHAP Paper: <a href="https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf">https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf</a></p><p>Introduction to probability and statistics for engineers and scientists. <a href="https://www.amazon.com.br/dp/B007ZBZN9U/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1">https://www.amazon.com.br/dp/B007ZBZN9U/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/data-science/">data science</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/data-science/" class="post-tag no-text-decoration" >data science</a> <a href="/tags/feature-engineering/" class="post-tag no-text-decoration" >feature engineering</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >machine learning</a> <a href="/tags/python/" class="post-tag no-text-decoration" >python</a> <a href="/tags/pandas/" class="post-tag no-text-decoration" >pandas</a> <a href="/tags/scikit-learn/" class="post-tag no-text-decoration" >scikit-learn</a> <a href="/tags/tensorflow/" class="post-tag no-text-decoration" >tensorflow</a> <a href="/tags/keras/" class="post-tag no-text-decoration" >keras</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted me-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Keras%20101:%20A%20simple%20Neural%20Network%20for%20House%20Pricing%20regression%20-%20Rodrigo%20Bressan&url=https%3A%2F%2Fbressan.xyz%2Fposts%2Fkeras-101-boston%2F" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter" > <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Keras%20101:%20A%20simple%20Neural%20Network%20for%20House%20Pricing%20regression%20-%20Rodrigo%20Bressan&u=https%3A%2F%2Fbressan.xyz%2Fposts%2Fkeras-101-boston%2F" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook" > <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fbressan.xyz%2Fposts%2Fkeras-101-boston%2F&text=Keras%20101:%20A%20simple%20Neural%20Network%20for%20House%20Pricing%20regression%20-%20Rodrigo%20Bressan" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram" > <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fbressan.xyz%2Fposts%2Fkeras-101-boston%2F" data-bs-toggle="tooltip" data-bs-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin" > <i class="fa-fw fab fa-linkedin"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/lynis-audit/">Navigating HIPAA compliance on the Cloud with Lynis</a><li class="text-truncate lh-lg"> <a href="/posts/kopf-k8s-operator/">Simplifying Kubernetes Operators with Kopf: A Quick Guide</a><li class="text-truncate lh-lg"> <a href="/posts/k8s-commands/">Essential kubectl Commands</a><li class="text-truncate lh-lg"> <a href="/posts/nlp-basics/">NLP with Python: A Beginner's Guide</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/data-science/">data science</a> <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a> <a class="post-tag btn btn-outline-primary" href="/tags/cloud/">cloud</a> <a class="post-tag btn btn-outline-primary" href="/tags/feature-engineering/">feature engineering</a> <a class="post-tag btn btn-outline-primary" href="/tags/keras/">keras</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning/">machine learning</a> <a class="post-tag btn btn-outline-primary" href="/tags/pandas/">pandas</a> <a class="post-tag btn btn-outline-primary" href="/tags/scikit-learn/">scikit-learn</a> <a class="post-tag btn btn-outline-primary" href="/tags/tensorflow/">tensorflow</a> <a class="post-tag btn btn-outline-primary" href="/tags/devops/">devops</a></div></section></div><section id="toc-wrapper" class="ps-0 pe-4 mb-5"><h2 class="panel-heading ps-3 pt-2 mb-2">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/entity-embeddings/" class="post-preview card h-100"><div class="card-body"> <time class="small" data-ts="1640008800" data-df="ll" > Dec 20, 2021 </time><h4 class="pt-0 my-2">Enhancing Categorical Features with Entity Embeddings</h4><div class="text-muted small"><p> Let’s talk about selling beers. Let’s suppose you are the owner of a pub, and you would like to predict how many beers your establishment is going to sell on a given day based on two variables: ...</p></div></div></a></article><article class="col"> <a href="/posts/shap/" class="post-preview card h-100"><div class="card-body"> <time class="small" data-ts="1642255200" data-df="ll" > Jan 15, 2022 </time><h4 class="pt-0 my-2">Elucidating Machine Learning models with SHAP values</h4><div class="text-muted small"><p> The problem During data analysis, we may only look at metrics such as accuracy/precision/recall and worry about not overfitting our data. But are those the only ones that matter? When it comes to...</p></div></div></a></article><article class="col"> <a href="/posts/keras-multi-output/" class="post-preview card h-100"><div class="card-body"> <time class="small" data-ts="1647352800" data-df="ll" > Mar 15, 2022 </time><h4 class="pt-0 my-2">Building a multi-output Convolutional Neural Network with Keras</h4><div class="text-muted small"><p> In this post, we will be exploring the Keras functional API in order to build a multi-output Deep Learning model. We will show how to train a single model that is capable of predicting three distin...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/shap/" class="btn btn-outline-primary" aria-label="Older" ><p>Elucidating Machine Learning models with SHAP values</p></a> <a href="/posts/keras-multi-output/" class="btn btn-outline-primary" aria-label="Newer" ><p>Building a multi-output Convolutional Neural Network with Keras</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p> © <time>2025</time> <a href="https://github.com/rodrigobressan">Rodrigo Bressan</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>See something wrong? Want to contribute? <a href="https://github.com/rodrigobressan/rodrigobressan.github.io/tree/master/_posts">Edit this page 📝</href></p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/data-science/">data science</a> <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a> <a class="post-tag btn btn-outline-primary" href="/tags/cloud/">cloud</a> <a class="post-tag btn btn-outline-primary" href="/tags/feature-engineering/">feature engineering</a> <a class="post-tag btn btn-outline-primary" href="/tags/keras/">keras</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning/">machine learning</a> <a class="post-tag btn btn-outline-primary" href="/tags/pandas/">pandas</a> <a class="post-tag btn btn-outline-primary" href="/tags/scikit-learn/">scikit-learn</a> <a class="post-tag btn btn-outline-primary" href="/tags/tensorflow/">tensorflow</a> <a class="post-tag btn btn-outline-primary" href="/tags/devops/">devops</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask"></div><script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.1/dist/jquery.min.js,npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.9/dayjs.min.js,npm/dayjs@1.11.9/locale/en.min.js,npm/dayjs@1.11.9/plugin/relativeTime.min.js,npm/dayjs@1.11.9/plugin/localizedFormat.min.js,npm/tocbot@4.21.1/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/unregister.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-5B9QC89SHN"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-5B9QC89SHN'); }); </script> <script> /* Note: dependent library will be loaded in `js-selector.html` */ SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5"></p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
